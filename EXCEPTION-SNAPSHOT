# SNAPSHOT — CST/CDT; global-date planning; real runtime timestamps (hh:mm:ss) in logs.
# Success path writes snapshot first, then logs D/I/U (counts can be 0). Failure writes a single 'E' row.
# This version guarantees start_time, end_time, insert_timestamp are TIMESTAMPs in America/Chicago.

from __future__ import annotations
from datetime import datetime, date
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import traceback
import pytz

from pyspark.sql import SparkSession, functions as F, types as T, Window
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.utils import AnalysisException

# ------------------------
# Spark session & configs
# ------------------------
spark = SparkSession.builder.appName("Apply Snapshot Tables - Global Dates (CST)").getOrCreate()

# Force CST/CDT for Spark session
spark.conf.set("spark.sql.session.timeZone", "America/Chicago")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# ------------------------
# Constants
# ------------------------
tz = pytz.timezone("America/Chicago")
LOCAL_TZ_NAME = "America/Chicago"

CATALOG = "MHMR_LAKEHOUSE"
SNAPSHOT_SCHEMA = f"{CATALOG}.SNAPSHOT"
STAGING_SCHEMA = "staging"
TABLES_LIST = "TABLES_LIST"  # columns: TABLE_NAME, KEY_COLUMN (comma-separated keys)

# If staging timestamps are UTC and must be localized to CST/CDT when landing in snapshot, set True.
STAGING_TIMESTAMPS_ARE_UTC = True

MAX_WORKERS = 4
MAX_RETRIES = 2
SLEEP_BETWEEN_RETRIES = 5

# ------------------------
# Utilities
# ------------------------
def _ts() -> str:
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def stg_tbl(table: str) -> str:
    return f"{STAGING_SCHEMA}.stg_{table}"

def snap_tbl(table: str) -> str:
    return f"{SNAPSHOT_SCHEMA}.{table}_SNAPSHOT"

def _resolve_insert_ts_col(df: DataFrame) -> str:
    cols = {c.lower(): c for c in df.columns}
    for c in ("insert_timestamp", "insert_ts"):
        if c in cols:
            return cols[c]
    raise ValueError("Insert timestamp column not found. Expected one of: insert_timestamp, insert_ts")

def _resolve_status_col(df: DataFrame) -> str:
    for c in df.columns:
        if c.lower() == "status":
            return c
    raise ValueError("STATUS column not found in staging.")

def _to_cst_date_from_ts(col_name: str):
    # Convert timestamp column to CST/CDT date for business-day filtering.
    if STAGING_TIMESTAMPS_ARE_UTC:
        return F.to_date(F.from_utc_timestamp(F.col(col_name), LOCAL_TZ_NAME))
    else:
        return F.to_date(F.col(col_name))

def _staging_on_date(df: DataFrame, d: date) -> DataFrame:
    ts = _resolve_insert_ts_col(df)
    return df.where(_to_cst_date_from_ts(ts) == F.lit(str(d)).cast("date"))

def _convert_to_cst(df: DataFrame, ts_cols: List[str]) -> DataFrame:
    out = df
    for c in ts_cols:
        if c in out.columns:
            out = out.withColumn(c, F.from_utc_timestamp(F.col(c), LOCAL_TZ_NAME))
    return out

# Build Spark TIMESTAMPs that are truly America/Chicago by pinning the instant at UTC,
# then converting to LOCAL_TZ_NAME. This avoids UI/session ambiguities.
def _lit_ts(dt_py: datetime):
    utc_str = dt_py.astimezone(pytz.UTC).strftime("%Y-%m-%d %H:%M:%S")
    return F.from_utc_timestamp(F.to_timestamp(F.lit(utc_str)), LOCAL_TZ_NAME)

# ------------------------
# Logging (daily scratchpad → FINAL with first-seen policy)
# ------------------------
snap_log_schema = T.StructType([
    T.StructField("load_date", T.DateType()),
    T.StructField("table_name", T.StringType()),
    T.StructField("status", T.StringType()),      # 'D','I','U' (success) or 'E' (failure)
    T.StructField("count", T.LongType()),
    T.StructField("error_message", T.StringType()),
    T.StructField("insert_timestamp", T.TimestampType()),  # actual write time (CST/CDT)
    T.StructField("start_time", T.TimestampType()),        # real runtime (CST/CDT)
    T.StructField("end_time", T.TimestampType()),          # real runtime (CST/CDT)
    T.StructField("load_type", T.StringType()),            # "incremental"
])

def ensure_snapshot_log_tables_exist() -> None:
    print(f"[{_ts()}] Ensuring snapshot log tables exist ...")
    spark.sql("CREATE SCHEMA IF NOT EXISTS LOGS")
    try:
        spark.read.table("LOGS.snapshot_daily_log")
    except Exception:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    try:
        spark.read.table("LOGS.snapshot_final_log")
    except Exception:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")

# One-time/recurring guard: upgrade legacy DATE columns to TIMESTAMP so time-of-day is preserved.
def _ensure_log_columns_are_timestamps():
    for tbl in ["LOGS.snapshot_daily_log", "LOGS.snapshot_final_log"]:
        df = spark.read.table(tbl)
        fields = {f.name.lower(): f.dataType.simpleString() for f in df.schema.fields}
        missing = [c for c in ["insert_timestamp", "start_time", "end_time"] if c not in fields]
        if missing:
            raise Exception(f"{tbl} missing required columns: {missing}")
        needs_alter = any(fields[c] != "timestamp" for c in ["insert_timestamp", "start_time", "end_time"])
        if needs_alter:
            print(f"[{_ts()}] Altering {tbl} columns to TIMESTAMP for time precision.")
            spark.sql(f"ALTER TABLE {tbl} ALTER COLUMN insert_timestamp TYPE TIMESTAMP")
            spark.sql(f"ALTER TABLE {tbl} ALTER COLUMN start_time TYPE TIMESTAMP")
            spark.sql(f"ALTER TABLE {tbl} ALTER COLUMN end_time TYPE TIMESTAMP")

def reset_daily_log() -> None:
    ensure_snapshot_log_tables_exist()
    _ensure_log_columns_are_timestamps()
    print(f"[{_ts()}] Truncating LOGS.snapshot_daily_log ...")
    spark.sql("TRUNCATE TABLE LOGS.snapshot_daily_log")

def append_snapshot_daily_log(table_name: str,
                              load_date: date,
                              status: str,
                              cnt: int,
                              start_time_py: datetime | None = None,
                              end_time_py: datetime | None = None,
                              error_message: str | None = None) -> None:
    now_py = datetime.now(tz)
    insert_ts = _lit_ts(now_py)
    start_ts  = _lit_ts(start_time_py or now_py)
    end_ts    = _lit_ts(end_time_py   or now_py)

    row_df = (spark.range(1).select(
        F.lit(load_date).cast("date").alias("load_date"),
        F.lit(table_name).alias("table_name"),
        F.lit(status).alias("status"),
        F.lit(int(cnt or 0)).cast("long").alias("count"),
        F.lit(error_message).cast("string").alias("error_message"),
        insert_ts.alias("insert_timestamp"),
        start_ts.alias("start_time"),
        end_ts.alias("end_time"),
        F.lit("incremental").alias("load_type"),
    ))
    print(f"[{_ts()}] LOG += {table_name} {load_date} status={status} count={cnt} "
          f"(start/end/insert have real CST/CDT times)")
    row_df.write.mode("append").saveAsTable("LOGS.snapshot_daily_log")

def consolidate_snapshot_logs_to_final() -> None:
    print(f"[{_ts()}] Consolidating snapshot daily -> final (first-seen policy) ...")
    try:
        daily = spark.read.table("LOGS.snapshot_daily_log")
    except Exception:
        print(f"[{_ts()}] No daily log found; nothing to consolidate.")
        return

    if daily.rdd.isEmpty():
        print(f"[{_ts()}] Daily log empty; truncating and exiting consolidation.")
        spark.sql("TRUNCATE TABLE LOGS.snapshot_daily_log")
        return

    try:
        final_ = spark.read.table("LOGS.snapshot_final_log")
    except Exception:
        final_ = spark.createDataFrame([], snap_log_schema)

    keys = ["load_date", "table_name", "status"]
    existing_keys = final_.select(*keys).distinct()
    new_only = (daily.alias("d")
                    .join(existing_keys.alias("f"), on=keys, how="left_anti")
                    .select("d.*"))

    print(f"[{_ts()}] FINAL before={final_.count()} | DAILY new={new_only.count()}")
    final_updated = final_.unionByName(new_only)
    final_updated.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
    spark.sql("TRUNCATE TABLE LOGS.snapshot_daily_log")
    print(f"[{_ts()}] Consolidation done. FINAL now={spark.read.table('LOGS.snapshot_final_log').count()} rows")

# ------------------------
# Global date helpers
# ------------------------
def union_staging_dates_across_tables(tables: List[Dict[str, str]]) -> List[date]:
    all_dates_df = None
    for r in tables:
        table = r["TABLE_NAME"]
        try:
            stg = spark.read.table(stg_tbl(table))
        except AnalysisException:
            continue
        ts_col = _resolve_insert_ts_col(stg)
        ddf = stg.select(_to_cst_date_from_ts(ts_col).alias("load_date")).distinct()
        all_dates_df = ddf if all_dates_df is None else all_dates_df.unionByName(ddf)

    if all_dates_df is None or all_dates_df.rdd.isEmpty():
        today = datetime.now(tz).date()
        print(f"[{_ts()}] No staging dates found; defaulting to today={today}")
        return [today]

    return [r["load_date"] for r in all_dates_df.select("load_date").distinct().orderBy("load_date").collect()]

def dates_to_process_for_table(table_name: str, candidate_dates: List[date]) -> List[date]:
    try:
        fl = spark.read.table("LOGS.snapshot_final_log")
    except Exception:
        fl = None

    done_set = set()
    if fl is not None:
        done = (fl.filter((F.col("table_name") == table_name) & (F.col("status").isin("I","U","D")))
                  .select("load_date").distinct())
        done_set = {r["load_date"] for r in done.collect()}

    return [d for d in candidate_dates if d not in done_set]

# ------------------------
# CDC apply per (table, date)
# ------------------------
def apply_snapshot_for_table_and_date(table_name: str, key_cols: List[str], d: date) -> None:
    op_start = datetime.now(tz)
    print(f"[{_ts()}] === APPLY SNAPSHOT: table={table_name} | date={d} ===")
    try:
        stg_df = spark.read.table(stg_tbl(table_name))
        print(f"[{_ts()}] Read staging: {stg_tbl(table_name)}")
        status_col = _resolve_status_col(stg_df)
        stg_day = _staging_on_date(stg_df, d)

        day_count = stg_day.count()
        print(f"[{_ts()}] Staging rows @ {d}: {day_count}")

        snap_table = snap_tbl(table_name)
        temp_table = snap_table.replace("_SNAPSHOT", "_TEMP")

        # Ensure snapshot exists (seed empty schema from staging if missing)
        try:
            snap_df = spark.read.table(snap_table)
        except AnalysisException:
            print(f"[{_ts()}] Seed empty snapshot for {table_name}.")
            stg_df.limit(0).write.mode("overwrite").saveAsTable(snap_table)
            snap_df = spark.read.table(snap_table)

        snap_cols = snap_df.columns

        if day_count == 0:
            op_end = datetime.now(tz)
            print(f"[{_ts()}] No rows for {table_name} on {d}. Logging zeros.")
            append_snapshot_daily_log(table_name, d, "D", 0, start_time_py=op_start, end_time_py=op_end)
            append_snapshot_daily_log(table_name, d, "I", 0, start_time_py=op_start, end_time_py=op_end)
            append_snapshot_daily_log(table_name, d, "U", 0, start_time_py=op_start, end_time_py=op_end)
            return

        # DELETE
        start_d = datetime.now(tz)
        d_keys = (stg_day.filter(F.upper(F.col(status_col)) == "D")
                         .select(*key_cols).dropDuplicates())
        d_matches = (snap_df.alias("t")
                           .join(d_keys.alias("d"),
                                 on=[F.col(f"t.{k}") == F.col(f"d.{k}") for k in key_cols],
                                 how="inner"))
        d_count = d_matches.count()
        end_d = datetime.now(tz)
        print(f"[{_ts()}] DELETE matches: {d_count}")

        # Remove any touched keys so I/U can re-add fresh
        touched_keys = stg_day.select(*key_cols).dropDuplicates()
        base_df = (snap_df.alias("t")
                          .join(touched_keys.alias("s"),
                                on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
                                how="left_anti"))

        # INSERT / UPDATE frames
        start_i = datetime.now(tz)
        i_df_raw = stg_day.filter(F.upper(F.col(status_col)) == "I").select(*snap_cols)
        start_u = datetime.now(tz)
        u_df_raw = stg_day.filter(F.upper(F.col(status_col)) == "U").select(*snap_cols)

        if STAGING_TIMESTAMPS_ARE_UTC:
            ts_like = [c for c in snap_cols if c.lower() in ("insert_ts","insert_timestamp","update_ts","load_ts")]
            i_df = _convert_to_cst(i_df_raw, ts_like)
            u_df = _convert_to_cst(u_df_raw, ts_like)
        else:
            i_df, u_df = i_df_raw, u_df_raw

        i_count = i_df.count(); end_i = datetime.now(tz)
        u_count = u_df.count(); end_u = datetime.now(tz)

        final_df = (base_df
                    .unionByName(i_df, allowMissingColumns=True)
                    .unionByName(u_df, allowMissingColumns=True))

        # If no net effect, log zeros with real times and skip swap
        if d_count == 0 and i_count == 0 and u_count == 0:
            print(f"[{_ts()}] No effective changes. Logging zeros; no swap.")
            append_snapshot_daily_log(table_name, d, "D", 0, start_time_py=start_d, end_time_py=end_d)
            append_snapshot_daily_log(table_name, d, "I", 0, start_time_py=start_i, end_time_py=end_i)
            append_snapshot_daily_log(table_name, d, "U", 0, start_time_py=start_u, end_time_py=end_u)
            return

        # Write TEMP and atomically swap
        print(f"[{_ts()}] Writing TEMP -> {temp_table}")
        final_df.write.mode("overwrite").saveAsTable(temp_table)
        print(f"[{_ts()}] Swapping TEMP -> SNAPSHOT {snap_table}")
        spark.sql(f"DROP TABLE IF EXISTS {snap_table}")
        spark.sql(f"ALTER TABLE {temp_table} RENAME TO {snap_table}")

        # Success logs
        append_snapshot_daily_log(table_name, d, "D", d_count, start_time_py=start_d, end_time_py=end_d)
        append_snapshot_daily_log(table_name, d, "I", i_count, start_time_py=start_i, end_time_py=end_i)
        append_snapshot_daily_log(table_name, d, "U", u_count, start_time_py=start_u, end_time_py=end_u)

        print(f"[{_ts()}] COMPLETE: {table_name} @ {d}  D={d_count}, I={i_count}, U={u_count}")

    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        op_end = datetime.now(tz)
        append_snapshot_daily_log(table_name, d, "E", 0, start_time_py=op_start, end_time_py=op_end, error_message=msg)
        print(f"[{_ts()}] FAILED: {table_name} @ {d} -> {msg}")
        raise

# ------------------------
# Orchestration
# ------------------------
def _apply_with_retry_for_date(table_name: str, key_cols: List[str], d: date) -> None:
    attempts = 0
    while True:
        try:
            print(f"[{_ts()}] Start {table_name} {d} (attempt {attempts+1})")
            apply_snapshot_for_table_and_date(table_name, key_cols, d)
            print(f"[{_ts()}] Finished {table_name} {d}")
            return
        except Exception:
            attempts += 1
            if attempts > MAX_RETRIES:
                print(f"[{_ts()}] FINAL FAILURE {table_name} {d} after {attempts} attempts")
                return
            print(f"[{_ts()}] RETRY in {SLEEP_BETWEEN_RETRIES}s for {table_name} {d}")
            time.sleep(SLEEP_BETWEEN_RETRIES)

def process_all_snapshot() -> None:
    print(f"[{_ts()}] ========= Start snapshot apply (global dates; CST/CDT) =========")
    ensure_snapshot_log_tables_exist()
    _ensure_log_columns_are_timestamps()
    reset_daily_log()  # safe to truncate after ensuring TIMESTAMP types

    tables_df = spark.read.table(TABLES_LIST).select("TABLE_NAME", "KEY_COLUMN")
    all_rows = [r.asDict() for r in tables_df.collect()]
    print(f"[{_ts()}] Tables to scan: {len(all_rows)}")

    global_dates = union_staging_dates_across_tables(all_rows)
    print(f"[{_ts()}] Global candidate dates: {', '.join(map(str, global_dates))}")

    plan = {}
    total_pending = 0
    for r in all_rows:
        table = r["TABLE_NAME"]
        key_cols = [c.strip() for c in r["KEY_COLUMN"].split(",") if c.strip()]
        pend = dates_to_process_for_table(table, global_dates)
        plan[table] = {"keys": key_cols, "pending": pend}
        total_pending += len(pend)
        if pend:
            print(f"[{_ts()}] PLAN {table}: {', '.join(map(str, pend))}")
        else:
            print(f"[{_ts()}] PLAN {table}: no pending dates.")

    if total_pending == 0:
        print(f"[{_ts()}] Nothing pending. Consolidating and exiting.")
        consolidate_snapshot_logs_to_final()
        print(f"[{_ts()}] ========= Done (no work) =========")
        return

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = []
        for table, info in plan.items():
            dates = info["pending"]
            if not dates:
                continue
            def work(table=table, key_cols=info["keys"], dates=dates):
                print(f"[{_ts()}] >>> Processing {table}: {', '.join(map(str, dates))}")
                for d in dates:
                    _apply_with_retry_for_date(table, key_cols, d)
            futures.append(ex.submit(work))
        for f in as_completed(futures):
            f.result()

    consolidate_snapshot_logs_to_final()
    print(f"[{_ts()}] ========= All snapshot processing complete =========")

# Entry point
process_all_snapshot()
