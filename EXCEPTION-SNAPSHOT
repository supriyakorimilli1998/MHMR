def apply_snapshot_for_table(table_name, key_cols):
    start_table = datetime.now(tz)
    print(f"[{_ts()}] APPLY SNAPSHOT (today only): {table_name} | keys={key_cols}")

    try:
        stg_full = spark.read.table(stg_tbl(table_name))
        stg_today = _staging_today(stg_full)  # filter to TODAY
        status_col = _resolve_status_col(stg_full)

        # Ensure snapshot exists (handles first run + empty-today case)
        boot = ensure_snapshot_exists_from_staging(table_name)

        if stg_today.rdd.isEmpty():
            print(f"[{_ts()}] No staging rows for today in {stg_tbl(table_name)}. Logging zeros and continuing.")
            t0 = datetime.now(tz)
            for s in ("D","I","U"):
                complete_snapshot_daily_log(table_name, s, 0, t0, None)
            return

        if boot:
            print(f"[{_ts()}] Snapshot created/bootstrapped for {table_name}. No further apply needed in this run.")
            return

        snap = spark.read.table(snap_tbl(table_name))

        # Keep a canonical column order from staging (and drop any temp cols)
        all_cols = [c for c in stg_today.columns if c != "_ins_local_dt"]

        # Prepare key sets
        stg_keys_all = stg_today.select(*key_cols).dropDuplicates()
        snap_keys    = snap.select(*key_cols).dropDuplicates()

        # Split by STATUS
        stg_I = stg_today.filter(F.upper(F.col(status_col)) == F.lit("I"))
        stg_U = stg_today.filter(F.upper(F.col(status_col)) == F.lit("U"))

        # --- DELETES (left-anti on ANY today's staging keys)
        start_d = datetime.now(tz)
        del_keys = (snap_keys.alias("t")
                    .join(stg_keys_all.alias("s"),
                          on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
                          how="left_anti"))
        d_count = del_keys.count()
        print(f"[{_ts()}] {table_name}: DELETE keys count = {d_count}")
        if d_count > 0:
            del_keys.createOrReplaceTempView("del_keys_vw")
            cond = " AND ".join([f"t.{k}=d.{k}" for k in key_cols])
            spark.sql(f"""
                DELETE FROM {snap_tbl(table_name)} AS t
                WHERE EXISTS (SELECT 1 FROM del_keys_vw d WHERE {cond})
            """)
        complete_snapshot_daily_log(table_name, "D", d_count, start_d, None)

        # --- INSERTS (STATUS='I' anti snapshot) -> select ONLY staging columns
        start_i = datetime.now(tz)
        i_src = (stg_I.alias("s")
                 .join(snap_keys.alias("t"),
                       on=[F.col(f"s.{k}") == F.col(f"t.{k}") for k in key_cols],
                       how="left_anti")
                 .select([F.col(f"s.{c}").alias(c) for c in all_cols]))
        i_count = i_src.count()
        print(f"[{_ts()}] {table_name}: INSERT rows (STATUS='I' anti snapshot) = {i_count}")

        # --- UPDATES (STATUS='U' inner snapshot) -> select ONLY staging columns
        start_u = datetime.now(tz)
        u_src = (stg_U.alias("s")
                 .join(snap_keys.alias("t"),
                       on=[F.col(f"s.{k}") == F.col(f"t.{k}") for k in key_cols],
                       how="inner")
                 .select([F.col(f"s.{c}").alias(c) for c in all_cols]))
        u_count = u_src.count()
        print(f"[{_ts()}] {table_name}: UPDATE rows (STATUS='U' inner snapshot) = {u_count}")

        if i_count == 0 and u_count == 0:
            print(f"[{_ts()}] {table_name}: Nothing to MERGE (I=0, U=0).")
            complete_snapshot_daily_log(table_name, "I", 0, start_i, None)
            complete_snapshot_daily_log(table_name, "U", 0, start_u, None)
            print(f"[{_ts()}] DONE {table_name} | D={d_count}, I=0, U=0 | Elapsed={(datetime.now(tz)-start_table)}")
            return

        # Build one aligned source DF (no column-count mismatch)
        src_df = (i_src.withColumn("SRC_OP", F.lit("I"))
                        .unionByName(u_src.withColumn("SRC_OP", F.lit("U")), allowMissingColumns=False))
        src_df.createOrReplaceTempView("src_union_vw")

        on = " AND ".join([f"t.{k}=s.{k}" for k in key_cols])
        set_clause  = ", ".join([f"t.{c}=s.{c}" for c in all_cols])
        insert_cols = ", ".join(all_cols)
        insert_vals = ", ".join([f"s.{c}" for c in all_cols])

        spark.sql(f"""
            MERGE INTO {snap_tbl(table_name)} AS t
            USING (SELECT * FROM src_union_vw) AS s
            ON {on}
            WHEN MATCHED AND s.SRC_OP = 'U' THEN UPDATE SET {set_clause}
            WHEN NOT MATCHED AND s.SRC_OP = 'I' THEN INSERT ({insert_cols}) VALUES ({insert_vals})
        """)

        complete_snapshot_daily_log(table_name, "I", i_count, start_i, None)
        complete_snapshot_daily_log(table_name, "U", u_count, start_u, None)

        print(f"[{_ts()}] DONE {table_name} | D={d_count}, I={i_count}, U={u_count} | Elapsed={(datetime.now(tz)-start_table)}")

    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        complete_snapshot_daily_log(table_name, "U", 0, start_table, msg)
        print(f"[{_ts()}] FAILED {table_name}: {msg}")
        raise
