# FINAL SNAPSHOT APPLY SCRIPT (LOG-DRIVEN, STATUS-AWARE)

from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.window import Window
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, traceback, os, time

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Apply Snapshot Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")

# === CONFIG ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()

CATALOG         = "MHMR_LAKEHOUSE"
SNAPSHOT_SCHEMA = f"{CATALOG}.SNAPSHOT"
STAGING_SCHEMA  = "staging"
TABLES_LIST     = "TABLES_LIST"

MAX_WORKERS = 4
MAX_RETRIES = 2
SLEEP_BETWEEN_RETRIES = 5
SNAP_LOG_LOCK = "/tmp/snapshot_logs_legacy.lock"

def _ts(): return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

# === LOG SCHEMA ===
snap_log_schema = T.StructType([
    T.StructField("load_date",        T.DateType()),
    T.StructField("table_name",       T.StringType()),
    T.StructField("status",           T.StringType()),
    T.StructField("count",            T.LongType()),
    T.StructField("error_message",    T.StringType()),
    T.StructField("insert_timestamp", T.TimestampType()),
    T.StructField("start_time",       T.TimestampType()),
    T.StructField("end_time",         T.TimestampType()),
    T.StructField("load_type",        T.StringType()),
])

# === LOCKING ===
def _acquire_lock(path=SNAP_LOG_LOCK, timeout=30, interval=0.2):
    start = time.time()
    while os.path.exists(path):
        if time.time() - start > timeout:
            return False
        time.sleep(interval)
    with open(path, "w") as f:
        f.write("locked")
    return True

def _release_lock(path=SNAP_LOG_LOCK):
    try:
        if os.path.exists(path):
            os.remove(path)
    except: pass

# === LOGGING ===
def ensure_snapshot_log_tables_exist():
    spark.sql("CREATE SCHEMA IF NOT EXISTS LOGS")
    try:
        spark.read.table("LOGS.snapshot_daily_log")
    except:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    try:
        spark.read.table("LOGS.snapshot_final_log")
    except:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")

def _legacy_write_snapshot_daily(row_df):
    if not _acquire_lock():
        return
    try:
        try:
            existing = spark.read.table("LOGS.snapshot_daily_log")
            final_df = existing.unionByName(row_df)
        except:
            final_df = row_df
        final_df.write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    finally:
        _release_lock()

def complete_snapshot_daily_log(table_name, status, cnt, start_time, error_message=None):
    end_time = datetime.now(tz)
    row_df = spark.createDataFrame([
        (Today_date, table_name, status, int(cnt or 0), error_message,
         end_time, start_time, end_time, "incremental")
    ], schema=snap_log_schema)
    _legacy_write_snapshot_daily(row_df)

def consolidate_snapshot_logs_to_final():
    try:
        daily_df = spark.read.table("LOGS.snapshot_daily_log")
    except: return

    try:
        final_df = spark.read.table("LOGS.snapshot_final_log")
    except:
        daily_df.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
        spark.sql("DROP TABLE IF EXISTS LOGS.snapshot_daily_log")
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
        return

    key_cols = ["load_date", "table_name", "status", "count", "error_message"]
    cond = [F.col(f"d.{c}") == F.col(f"f.{c}") for c in key_cols]
    to_add = daily_df.alias("d").join(final_df.select(*key_cols).alias("f"), on=cond, how="left_anti")
    final_union = final_df.unionByName(to_add)
    deduped = (final_union
               .withColumn("rn", F.row_number().over(Window.partitionBy(*key_cols).orderBy(F.col("end_time").desc())))
               .filter(F.col("rn") == 1).drop("rn"))
    deduped.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
    spark.sql("DROP TABLE IF EXISTS LOGS.snapshot_daily_log")
    spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")

# === HELPERS ===
def stg_tbl(table): return f"{STAGING_SCHEMA}.stg_{table}"
def snap_tbl(table): return f"{SNAPSHOT_SCHEMA}.{table}_SNAPSHOT"

def _resolve_insert_ts_col(df):
    cols = {c.lower(): c for c in df.columns}
    for name in ["insert_timestamp", "insert_ts"]:
        if name.lower() in cols:
            return cols[name.lower()]
    raise ValueError("No insert timestamp column found.")

def _resolve_status_col(df):
    for c in df.columns:
        if c.lower() == "status":
            return c
    raise ValueError("No STATUS column found.")

def _local_date_from_ts(ts_col_name):
    return F.to_date(F.from_utc_timestamp(F.col(ts_col_name), "America/Chicago"))

def _staging_today(df):
    ts_col = _resolve_insert_ts_col(df)
    return (df
            .withColumn("_ins_local_dt", _local_date_from_ts(ts_col))
            .filter(F.col("_ins_local_dt") == F.lit(str(Today_date)).cast("date"))
            .drop("_ins_local_dt"))

# === LOG-DRIVEN EXECUTION CONTROL ===
def _today_final_logs():
    try:
        return spark.read.table("LOGS.snapshot_final_log").filter(F.col("load_date") == Today_date)
    except:
        return spark.createDataFrame([], snap_log_schema)

def table_has_failure_today(table_name):
    df = _today_final_logs().filter((F.col("table_name") == table_name) & F.col("error_message").isNotNull())
    return not df.limit(1).rdd.isEmpty()

def table_has_all_success_today(table_name):
    df = _today_final_logs().filter((F.col("table_name") == table_name) & F.col("error_message").isNull())
    got = df.select("status").distinct().rdd.map(lambda r: r[0]).collect()
    return set(["I", "U", "D"]).issubset(set(got))

# === APPLY LOGIC ===
def apply_snapshot_for_table(table_name, key_cols):
    start_table = datetime.now(tz)

    try:
        stg_df = spark.read.table(stg_tbl(table_name))
        stg_today = _staging_today(stg_df)
        status_col = _resolve_status_col(stg_df)

        if stg_today.rdd.isEmpty():
            for s in ("D", "I", "U"):
                complete_snapshot_daily_log(table_name, s, 0, start_table, None)
            return

        snap_table = snap_tbl(table_name)
        temp_table = snap_table.replace("_SNAPSHOT", "_TEMP")

        try:
            snap_df = spark.read.table(snap_table)
        except AnalysisException:
            stg_df.limit(0).write.mode("overwrite").saveAsTable(snap_table)
            snap_df = spark.read.table(snap_table)

        all_cols = snap_df.columns

        # DELETE via left anti
        start_d = datetime.now(tz)
        stg_keys = stg_today.select(*key_cols).dropDuplicates()
        snap_filtered = snap_df.alias("t").join(
            stg_keys.alias("s"),
            on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
            how="inner"
        ).select("t.*")
        d_count = snap_df.count() - snap_filtered.count()
        complete_snapshot_daily_log(table_name, "D", d_count, start_d)

        # INSERT
        start_i = datetime.now(tz)
        i_df = stg_today.filter(F.upper(F.col(status_col)) == "I").select(*all_cols)
        i_count = i_df.count()
        complete_snapshot_daily_log(table_name, "I", i_count, start_i)

        # UPDATE
        start_u = datetime.now(tz)
        u_df = stg_today.filter(F.upper(F.col(status_col)) == "U").select(*all_cols)
        u_count = u_df.count()
        complete_snapshot_daily_log(table_name, "U", u_count, start_u)

        final_df = snap_filtered.unionByName(i_df).unionByName(u_df)
        final_df.write.mode("overwrite").saveAsTable(temp_table)

        spark.sql(f"DROP TABLE IF EXISTS {snap_table}")
        spark.sql(f"ALTER TABLE {temp_table} RENAME TO {snap_table}")

    except Exception as e:
        complete_snapshot_daily_log(table_name, "U", 0, start_table, str(e))
        raise

def apply_with_retry(row):
    table_name = row["TABLE_NAME"]
    key_cols = [c.strip() for c in row["KEY_COLUMN"].split(",")]
    attempts = 0
    while True:
        try:
            apply_snapshot_for_table(table_name, key_cols)
            return
        except Exception as e:
            attempts += 1
            if attempts > MAX_RETRIES:
                return
            time.sleep(SLEEP_BETWEEN_RETRIES)

# === MAIN DRIVER ===
def process_all_snapshot():
    ensure_snapshot_log_tables_exist()
    tables_df = spark.read.table(TABLES_LIST).select("TABLE_NAME", "KEY_COLUMN")
    all_rows = [r.asDict() for r in tables_df.collect()]

    failed, pending, success = [], [], []
    for r in all_rows:
        t = r["TABLE_NAME"]
        if table_has_failure_today(t): failed.append(r)
        elif table_has_all_success_today(t): success.append(r)
        else: pending.append(r)

    queue = failed if failed else pending
    if not queue:
        print("All tables processed today. Exiting.")
        return

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(apply_with_retry, r) for r in queue]
        for fut in as_completed(futures): fut.result()

    consolidate_snapshot_logs_to_final()

# === RUN ===
process_all_snapshot()
