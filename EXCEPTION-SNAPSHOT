# --- NEW: helper to create an empty snapshot from staging schema (first run, no data today) ---
def _create_empty_snapshot_from_staging_schema(table_name, stg_full_df):
    tgt = snap_tbl(table_name)
    empty_df = stg_full_df.limit(0)  # preserves schema, 0 rows
    empty_df.write.mode("overwrite").saveAsTable(tgt)

# === REPLACE ensure_snapshot_exists_from_staging WITH THIS VERSION ===
def ensure_snapshot_exists_from_staging(table_name):
    """
    Guarantee {SNAPSHOT}.{table}_SNAPSHOT exists.
    If staging has rows for TODAY -> bootstrap from those rows.
    If staging is empty for TODAY -> create an EMPTY snapshot from staging schema.
    Returns True if we created the snapshot (bootstrapped or empty), else False.
    """
    tgt = snap_tbl(table_name)
    try:
        spark.read.table(tgt)  # already exists
        return False
    except AnalysisException:
        stg_full = spark.read.table(stg_tbl(table_name))
        stg_today = _staging_today(stg_full)
        if stg_today.rdd.isEmpty():
            # First ever run with no data today: create empty snapshot and log zeros
            _create_empty_snapshot_from_staging_schema(table_name, stg_full)
            start_b = datetime.now(tz)
            complete_snapshot_daily_log(table_name, "D", 0, start_b, None)
            complete_snapshot_daily_log(table_name, "I", 0, start_b, None)
            complete_snapshot_daily_log(table_name, "U", 0, start_b, None)
            return True
        else:
            # Bootstrap from today's rows
            stg_today.write.mode("overwrite").saveAsTable(tgt)
            start_b = datetime.now(tz)
            cnt = stg_today.count()
            complete_snapshot_daily_log(table_name, "D", 0, start_b, None)
            complete_snapshot_daily_log(table_name, "I", cnt, start_b, None)
            complete_snapshot_daily_log(table_name, "U", 0, start_b, None)
            return True

# === REPLACE apply_snapshot_for_table WITH THIS VERSION ===
def apply_snapshot_for_table(table_name, key_cols):
    start_table = datetime.now(tz)
    print(f"[{_ts()}] APPLY SNAPSHOT (today only): {table_name} | keys={key_cols}")

    try:
        stg_full = spark.read.table(stg_tbl(table_name))
        stg = _staging_today(stg_full)  # filter to TODAY

        # Make sure snapshot exists (handles first run + empty-today case)
        boot = ensure_snapshot_exists_from_staging(table_name)
        if stg.rdd.isEmpty():
            # Nothing to do today: log zeros and return (no error)
            print(f"[{_ts()}] No staging rows for today in {stg_tbl(table_name)}. Logging zeros and continuing.")
            start0 = datetime.now(tz)
            complete_snapshot_daily_log(table_name, "D", 0, start0, None)
            complete_snapshot_daily_log(table_name, "I", 0, start0, None)
            complete_snapshot_daily_log(table_name, "U", 0, start0, None)
            return
        if boot:
            # Snapshot was just created (either empty or bootstrapped). If it was empty, we’re done;
            # if it was bootstrapped from today's rows, nothing else to apply.
            print(f"[{_ts()}] Snapshot created for {table_name}.")
            return

        # Normal path: apply D -> I -> U
        snap = spark.read.table(snap_tbl(table_name))

        stg_keys  = stg.select(*key_cols).dropDuplicates()
        snap_keys = snap.select(*key_cols).dropDuplicates()

        # --- DELETES: keys in SNAPSHOT but not in TODAY'S STAGING (left anti)
        start_d = datetime.now(tz)
        del_keys = snap_keys.alias("t").join(
            stg_keys.alias("s"),
            on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
            how="left_anti"
        )
        d_count = del_keys.count()
        if d_count > 0:
            del_keys.createOrReplaceTempView("del_keys_vw")
            cond = " AND ".join([f"t.{k}=d.{k}" for k in key_cols])
            spark.sql(f"""
                DELETE FROM {snap_tbl(table_name)} AS t
                WHERE EXISTS (SELECT 1 FROM del_keys_vw d WHERE {cond})
            """)
        complete_snapshot_daily_log(table_name, "D", d_count, start_d, None)

        # --- INSERTS: rows in TODAY'S STAGING not in SNAPSHOT (anti on keys)
        start_i = datetime.now(tz)
        i_src = stg.alias("s").join(
            snap_keys.alias("t"),
            on=[F.col(f"s.{k}") == F.col(f"t.{k}") for k in key_cols],
            how="left_anti"
        )
        i_count = i_src.count()

        # --- UPDATES: rows where keys exist in both (inner join)
        start_u = datetime.now(tz)
        u_src = stg.alias("s").join(
            snap_keys.alias("t"),
            on=[F.col(f"s.{k}") == F.col(f"t.{k}") for k in key_cols],
            how="inner"
        )
        u_count = u_src.count()

        # MERGE “as-is” for both matched and not matched
        if i_count > 0 or u_count > 0:
            stg.createOrReplaceTempView("src_all_vw")
            on = " AND ".join([f"t.{k}=s.{k}" for k in key_cols])
            all_cols = [c for c in stg.columns]
            set_clause  = ", ".join([f"t.{c}=s.{c}" for c in all_cols])
            insert_cols = ", ".join(all_cols)
            insert_vals = ", ".join([f"s.{c}" for c in all_cols])
            spark.sql(f"""
                MERGE INTO {snap_tbl(table_name)} AS t
                USING (SELECT * FROM src_all_vw) AS s
                ON {on}
                WHEN MATCHED THEN UPDATE SET {set_clause}
                WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})
            """)

        complete_snapshot_daily_log(table_name, "I", i_count, start_i, None)
        complete_snapshot_daily_log(table_name, "U", u_count, start_u, None)

        print(f"[{_ts()}] DONE {table_name} | D={d_count}, I={i_count}, U={u_count} | {datetime.now(tz)-start_table}")
    except Exception as e:
        # Only true failures get logged as errors; empty‑today is NOT an error.
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        complete_snapshot_daily_log(table_name, "U", 0, start_table, msg)
        raise
