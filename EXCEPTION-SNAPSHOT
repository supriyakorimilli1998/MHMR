from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, traceback, os, time

spark = SparkSession.builder.appName("Apply Snapshot Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()

CATALOG         = "MHMR_LAKEHOUSE"
SNAPSHOT_SCHEMA = f"{CATALOG}.SNAPSHOT"
STAGING_SCHEMA  = "staging"
TABLES_LIST     = "TABLES_LIST"

MAX_WORKERS = 4
MAX_RETRIES = 2
SLEEP_BETWEEN_RETRIES = 5
SNAP_LOG_LOCK = "/tmp/snapshot_logs_legacy.lock"

def _ts():
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")


snap_log_schema = T.StructType([
    T.StructField("load_date",        T.DateType()),
    T.StructField("table_name",       T.StringType()),
    T.StructField("status",           T.StringType()),
    T.StructField("count",            T.LongType()),
    T.StructField("error_message",    T.StringType()),
    T.StructField("insert_timestamp", T.TimestampType()),
    T.StructField("start_time",       T.TimestampType()),
    T.StructField("end_time",         T.TimestampType()),
    T.StructField("load_type",        T.StringType()),
])

def _acquire_lock(path=SNAP_LOG_LOCK, timeout=30, interval=0.2):
    print(f"[{_ts()}] [LOCK] Attempting to acquire log lock at {path} ...")
    start = time.time()
    while os.path.exists(path):
        if time.time() - start > timeout:
            print(f"[{_ts()}] [LOCK] Timeout waiting for {path}.")
            return False
        time.sleep(interval)
    with open(path, "w") as f:
        f.write("locked")
    print(f"[{_ts()}] [LOCK] Acquired {path}.")
    return True

def _release_lock(path=SNAP_LOG_LOCK):
    if os.path.exists(path):
        os.remove(path)
        print(f"[{_ts()}] [LOCK] Released {path}.")

def ensure_snapshot_log_tables_exist():
    print(f"[{_ts()}] Ensuring LOGS schema and snapshot log tables exist ...")
    spark.sql("CREATE SCHEMA IF NOT EXISTS LOGS")
    try:
        spark.read.table("LOGS.snapshot_daily_log")
    except:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    try:
        spark.read.table("LOGS.snapshot_final_log")
    except:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")

def _legacy_write_snapshot_daily(row_df):
    if not _acquire_lock():
        return
    try:
        try:
            existing = spark.read.table("LOGS.snapshot_daily_log")
            final_df = existing.unionByName(row_df)
        except:
            final_df = row_df
        final_df.write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    finally:
        _release_lock()

def complete_snapshot_daily_log(table_name, status, cnt, start_time, error_message=None):
    end_time = datetime.now(tz)
    row_df = spark.createDataFrame([
        (Today_date, table_name, status, int(cnt or 0), error_message,
         end_time, start_time, end_time, "incremental")
    ], schema=snap_log_schema)
    _legacy_write_snapshot_daily(row_df)

def consolidate_snapshot_logs_to_final():
    print(f"[{_ts()}] Consolidating daily log into final ...")
    try:
        daily_df = spark.read.table("LOGS.snapshot_daily_log")
    except:
        return
    try:
        final_df = spark.read.table("LOGS.snapshot_final_log")
    except:
        daily_df.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
        spark.sql("DROP TABLE IF EXISTS LOGS.snapshot_daily_log")
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
        return
    key_cols = ["load_date","table_name","status","end_time","count","error_message"]
    cond = [F.col(f"d.{c}") == F.col(f"f.{c}") for c in key_cols]
    to_add = daily_df.alias("d").join(final_df.select(*key_cols).alias("f"), cond, "left_anti")
    final_union = final_df.unionByName(to_add)
    final_union.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
    spark.sql("DROP TABLE IF EXISTS LOGS.snapshot_daily_log")
    spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")


def stg_tbl(table):   return f"{STAGING_SCHEMA}.stg_{table}"
def snap_tbl(table):  return f"{SNAPSHOT_SCHEMA}.{table}_SNAPSHOT"

def _resolve_insert_ts_col(df):
    cols = {c.lower(): c for c in df.columns}
    for name in ["insert_timestamp", "insert_ts"]:
        if name.lower() in cols:
            return cols[name.lower()]
    raise ValueError("Insert timestamp column not found.")

def _resolve_status_col(df):
    for c in df.columns:
        if c.lower() == "status":
            return c
    raise ValueError("STATUS column not found.")

def _local_date_from_ts(ts_col_name):
    return F.to_date(F.from_utc_timestamp(F.col(ts_col_name), "America/Chicago"))

def _staging_today(df):
    ts_col = _resolve_insert_ts_col(df)
    return df.withColumn("_ins_local_dt", _local_date_from_ts(ts_col)).filter(
        F.col("_ins_local_dt") == F.lit(str(Today_date)).cast("date")
    ).drop("_ins_local_dt")

def _today_final_logs():
    try:
        return spark.read.table("LOGS.snapshot_final_log").filter(F.col("load_date") == Today_date)
    except:
        return spark.createDataFrame([], snap_log_schema)

def table_has_success_today(table_name):
    df = _today_final_logs().filter(F.col("table_name") == table_name).filter(F.col("load_date") == Today_date)
    if df.rdd.isEmpty():
        return False
    return df.filter(F.col("error_message").isNotNull()).count() == 0

def table_has_failure_today(table_name):
    df = _today_final_logs().filter(F.col("table_name") == table_name)
    return not df.filter(F.col("error_message").isNotNull()).rdd.isEmpty()


def apply_snapshot_for_table(table_name, key_cols):
    start_table = datetime.now(tz)
    print(f"[{_ts()}] === APPLY SNAPSHOT: {table_name} ===")

    try:
        stg_df = spark.read.table(stg_tbl(table_name))
        stg_today = _staging_today(stg_df)
        status_col = _resolve_status_col(stg_df)

        if stg_today.rdd.isEmpty():
            print(f"[{_ts()}] No data for today in {stg_tbl(table_name)}.")
            t0 = datetime.now(tz)
            for s in ("D", "I", "U"):
                complete_snapshot_daily_log(table_name, s, 0, t0, None)
            return

        snap_table = snap_tbl(table_name)
        temp_table = snap_table.replace("_SNAPSHOT", "_TEMP")

        try:
            snap_df = spark.read.table(snap_table)
        except AnalysisException:
            print(f"[{_ts()}] Snapshot missing for {table_name}, creating empty schema.")
            stg_df.limit(0).write.mode("overwrite").saveAsTable(snap_table)
            snap_df = spark.read.table(snap_table)

        all_cols = snap_df.columns

        # === DELETES ===
        start_d = datetime.now(tz)
        stg_keys = stg_today.select(*key_cols).dropDuplicates()
        snap_filtered = snap_df.alias("t").join(
            stg_keys.alias("s"),
            on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
            how="inner"
        ).select("t.*")
        d_count = snap_df.count() - snap_filtered.count()
        print(f"[{_ts()}] DELETE count: {d_count}")
        complete_snapshot_daily_log(table_name, "D", d_count, start_d, None)

        # === INSERTS ===
        start_i = datetime.now(tz)
        i_df = stg_today.filter(F.upper(F.col(status_col)) == "I").select(*all_cols)
        i_count = i_df.count()
        print(f"[{_ts()}] INSERT count: {i_count}")
        complete_snapshot_daily_log(table_name, "I", i_count, start_i, None)

        # === UPDATES ===
        start_u = datetime.now(tz)
        u_df = stg_today.filter(F.upper(F.col(status_col)) == "U").select(*all_cols)
        u_count = u_df.count()
        print(f"[{_ts()}] UPDATE count: {u_count}")
        complete_snapshot_daily_log(table_name, "U", u_count, start_u, None)

        # === COMBINE & WRITE TEMP ===
        final_df = snap_filtered.unionByName(i_df).unionByName(u_df)
        print(f"[{_ts()}] Writing TEMP table: {temp_table}")
        final_df.write.mode("overwrite").saveAsTable(temp_table)

        # === ATOMIC SWAP ===
        print(f"[{_ts()}] Replacing SNAPSHOT: {snap_table}")
        spark.sql(f"DROP TABLE IF EXISTS {snap_table}")
        spark.sql(f"ALTER TABLE {temp_table} RENAME TO {snap_table}")

        print(f"[{_ts()}] ✅ COMPLETE: {table_name} | D={d_count}, I={i_count}, U={u_count}")

    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        complete_snapshot_daily_log(table_name, "U", 0, start_table, msg)
        print(f"[{_ts()}] ❌ FAILED: {table_name} | {msg}")
        raise


def apply_with_retry(row):
    table_name = row["TABLE_NAME"]
    key_cols = [c.strip() for c in row["KEY_COLUMN"].split(",")]
    attempts = 0
    while True:
        try:
            print(f"[{_ts()}] Starting: {table_name} (attempt {attempts+1})")
            apply_snapshot_for_table(table_name, key_cols)
            return
        except Exception as e:
            attempts += 1
            if attempts > MAX_RETRIES:
                print(f"[{_ts()}] FINAL FAILURE: {table_name}")
                return
            print(f"[{_ts()}] RETRYING {table_name} after failure. Waiting {SLEEP_BETWEEN_RETRIES}s")
            time.sleep(SLEEP_BETWEEN_RETRIES)



def process_all_snapshot():
    print(f"[{_ts()}] Starting snapshot apply process ...")
    ensure_snapshot_log_tables_exist()
    tables_df = spark.read.table(TABLES_LIST).select("TABLE_NAME","KEY_COLUMN")
    all_rows = [r.asDict() for r in tables_df.collect()]
    print(f"[{_ts()}] Total tables: {len(all_rows)}")

    failed, pending, success = [], [], []

    for row in all_rows:
        t = row["TABLE_NAME"]
        if table_has_failure_today(t):
            failed.append(row)
            print(f"[{_ts()}] Queue → FAILED: {t}")
        elif table_has_success_today(t):
            print(f"[{_ts()}] Skipping already successful table: {t}")
        else:
            pending.append(row)
            print(f"[{_ts()}] Queue → PENDING: {t}")

    queue = failed + pending
    if not queue:
        print(f"[{_ts()}] Nothing to process.")
        return

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(apply_with_retry, r) for r in queue]
        for f in as_completed(futures):
            f.result()

    consolidate_snapshot_logs_to_final()
    print(f"[{_ts()}] All snapshot processing complete.")


process_all_snapshot()
