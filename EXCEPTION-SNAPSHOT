# =================== SNAPSHOT APPLY (FOLLOWS STAGING TEMPLATE + TODAY FILTER) ===================
from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.utils import AnalysisException
from pyspark.sql.window import Window
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, traceback, os, time

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Apply Snapshot Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# === CONFIG ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()  # driver “today” in America/Chicago for filtering + logs

CATALOG = "MHMR_LAKEHOUSE"
SNAPSHOT_SCHEMA = f"{CATALOG}.SNAPSHOT"
STAGING_SCHEMA  = "staging"                 # stg_{TABLE_NAME}
TABLES_LIST     = "TABLES_LIST"             # TABLE_NAME, KEY_COLUMN (comma-separated)

MAX_WORKERS = 4
MAX_RETRIES = 2
SLEEP_BETWEEN_RETRIES = 5

def _ts():
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

# === SNAPSHOT LOG SCHEMA (legacy-compatible daily/final) ===
snap_log_schema = T.StructType([
    T.StructField("load_date",        T.DateType()),
    T.StructField("table_name",       T.StringType()),
    T.StructField("status",           T.StringType()),     # 'I' | 'U' | 'D'
    T.StructField("count",            T.LongType()),
    T.StructField("error_message",    T.StringType()),
    T.StructField("insert_timestamp", T.TimestampType()),
    T.StructField("start_time",       T.TimestampType()),
    T.StructField("end_time",         T.TimestampType()),
    T.StructField("load_type",        T.StringType()),     # 'incremental'
])

SNAP_LOG_LOCK = "/tmp/snapshot_logs_legacy.lock"

def _acquire_lock(path=SNAP_LOG_LOCK, timeout=30, interval=0.2):
    start = time.time()
    while os.path.exists(path):
        if time.time() - start > timeout:
            return False
        time.sleep(interval)
    with open(path, "w") as f:
        f.write("locked")
    return True

def _release_lock(path=SNAP_LOG_LOCK):
    try:
        if os.path.exists(path):
            os.remove(path)
    except:
        pass

def ensure_snapshot_log_tables_exist():
    print(f"[{_ts()}] Checking snapshot log tables existence (legacy mode)...")
    spark.sql("CREATE SCHEMA IF NOT EXISTS LOGS")
    try:
        spark.read.table("LOGS.snapshot_daily_log")
    except:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    try:
        spark.read.table("LOGS.snapshot_final_log")
    except:
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")

def _legacy_write_snapshot_daily(row_df):
    if not _acquire_lock():
        print(f"[{_ts()}] [LOCK] Could not acquire snapshot log lock. Skipping daily write.")
        return
    try:
        try:
            existing = spark.read.table("LOGS.snapshot_daily_log")
            final_df = existing.unionByName(row_df)
        except Exception:
            final_df = row_df
        final_df.write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    finally:
        _release_lock()

def complete_snapshot_daily_log(table_name, status, cnt, start_time, error_message=None):
    end_time = datetime.now(tz)
    row_df = spark.createDataFrame([
        (Today_date, table_name, status, int(cnt or 0), error_message,
         end_time, start_time, end_time, "incremental")
    ], schema=snap_log_schema)
    _legacy_write_snapshot_daily(row_df)

def consolidate_snapshot_logs_to_final():
    print(f"[{_ts()}] Consolidating LOGS.snapshot_daily_log -> LOGS.snapshot_final_log (legacy)…")
    try:
        daily_df = spark.read.table("LOGS.snapshot_daily_log")
    except:
        print(f"[{_ts()}] No snapshot_daily_log found; skipping consolidation.")
        return
    try:
        final_df = spark.read.table("LOGS.snapshot_final_log")
    except:
        daily_df.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
        spark.sql("DROP TABLE IF EXISTS LOGS.snapshot_daily_log")
        spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
        print(f"[{_ts()}] Consolidation done.")
        return
    key_cols = ["load_date","table_name","status","end_time","count","error_message"]
    cond = [F.col(f"d.{c}") == F.col(f"f.{c}") for c in key_cols]
    to_add = daily_df.alias("d").join(final_df.select(*key_cols).alias("f"), on=cond, how="left_anti")
    final_union = final_df.unionByName(to_add)
    final_union.write.mode("overwrite").saveAsTable("LOGS.snapshot_final_log")
    spark.sql("DROP TABLE IF EXISTS LOGS.snapshot_daily_log")
    spark.createDataFrame([], snap_log_schema).write.mode("overwrite").saveAsTable("LOGS.snapshot_daily_log")
    print(f"[{_ts()}] Snapshot final log updated and daily log reset (legacy).")

# === HELPERS ===
def stg_tbl(table):   return f"{STAGING_SCHEMA}.stg_{table}"
def snap_tbl(table):  return f"{SNAPSHOT_SCHEMA}.{table}_SNAPSHOT"

def _resolve_insert_ts_col(df):
    cols = {c.lower(): c for c in df.columns}
    for name in ["insert_timestamp", "insert_ts", "INSERT_TIMESTAMP", "INSERT_TS"]:
        if name.lower() in cols:
            return cols[name.lower()]
    raise ValueError("No insert timestamp column found (expected one of: insert_timestamp, insert_ts).")

def _staging_today(df):
    """
    Filter staging to today's rows only by converting its timestamp column to DATE
    and equating to Today_date (America/Chicago).
    """
    ts_col = _resolve_insert_ts_col(df)
    return (df
            .withColumn("_ins_dt", F.to_date(F.col(ts_col)))
            .filter(F.col("_ins_dt") == F.lit(str(Today_date)).cast("date"))
            .drop("_ins_dt"))

def ensure_snapshot_exists_from_staging(table_name):
    tgt = snap_tbl(table_name)
    try:
        spark.read.table(tgt)
        return False
    except AnalysisException:
        stg_full = spark.read.table(stg_tbl(table_name))
        stg = _staging_today(stg_full)
        if stg.rdd.isEmpty():
            # Nothing for today; still create empty snapshot to unblock downstream if desired
            # Or raise — here we raise to surface the issue.
            raise ValueError(f"No staging rows for today in {stg_tbl(table_name)}")
        stg.write.mode("overwrite").saveAsTable(tgt)
        start_b = datetime.now(tz)
        complete_snapshot_daily_log(table_name, "I", stg.count(), start_b, None)
        complete_snapshot_daily_log(table_name, "U", 0, start_b, None)
        complete_snapshot_daily_log(table_name, "D", 0, start_b, None)
        return True

# === STATUS DRIVERS (use snapshot_final_log for driver order) ===
def _today_final_logs():
    try:
        return spark.read.table("LOGS.snapshot_final_log").filter(F.col("load_date")==Today_date)
    except:
        return spark.createDataFrame([], snap_log_schema)

def table_has_failure_today(table_name):
    df = _today_final_logs().filter((F.col("table_name")==table_name) & F.col("error_message").isNotNull())
    return not df.limit(1).rdd.isEmpty()

def table_has_all_success_today(table_name):
    df = _today_final_logs().filter((F.col("table_name")==table_name) & F.col("error_message").isNull())
    got = df.select("status").distinct().rdd.map(lambda r: r[0]).collect()
    return set(["I","U","D"]).issubset(set(got))

# === CORE APPLY: DELETE (left-anti) → INSERTS → UPDATES, ALL FROM TODAY'S STAGING ===
def apply_snapshot_for_table(table_name, key_cols):
    start_table = datetime.now(tz)
    print(f"[{_ts()}] APPLY SNAPSHOT (today only): {table_name} | keys={key_cols}")

    try:
        stg_full = spark.read.table(stg_tbl(table_name))
        stg = _staging_today(stg_full)  # <<< filter to TODAY
        if stg.rdd.isEmpty():
            raise ValueError(f"No staging rows for today in {stg_tbl(table_name)}")

        boot = ensure_snapshot_exists_from_staging(table_name)
        if boot:
            print(f"[{_ts()}] Bootstrap complete for {table_name}.")
            return

        snap = spark.read.table(snap_tbl(table_name))

        stg_keys  = stg.select(*key_cols).dropDuplicates()
        snap_keys = snap.select(*key_cols).dropDuplicates()

        # --- DELETES: keys in SNAPSHOT but not in TODAY'S STAGING (left anti)
        start_d = datetime.now(tz)
        del_keys = snap_keys.alias("t").join(
            stg_keys.alias("s"),
            on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
            how="left_anti"
        )
        d_count = del_keys.count()
        if d_count > 0:
            del_keys.createOrReplaceTempView("del_keys_vw")
            cond = " AND ".join([f"t.{k}=d.{k}" for k in key_cols])
            spark.sql(f"""
                DELETE FROM {snap_tbl(table_name)} AS t
                WHERE EXISTS (SELECT 1 FROM del_keys_vw d WHERE {cond})
            """)
        complete_snapshot_daily_log(table_name, "D", d_count, start_d, None)

        # --- INSERTS: rows in TODAY'S STAGING not in SNAPSHOT (anti on keys)
        start_i = datetime.now(tz)
        i_src = stg.alias("s").join(
            snap_keys.alias("t"),
            on=[F.col(f"s.{k}") == F.col(f"t.{k}") for k in key_cols],
            how="left_anti"
        )
        i_count = i_src.count()

        # --- UPDATES: rows where keys exist in both (inner join) from TODAY'S STAGING
        start_u = datetime.now(tz)
        u_src = stg.alias("s").join(
            snap_keys.alias("t"),
            on=[F.col(f"s.{k}") == F.col(f"t.{k}") for k in key_cols],
            how="inner"
        )
        u_count = u_src.count()

        # MERGE “as-is” for both matched and not matched
        if i_count > 0 or u_count > 0:
            stg.createOrReplaceTempView("src_all_vw")
            on = " AND ".join([f"t.{k}=s.{k}" for k in key_cols])
            all_cols = [c for c in stg.columns]
            set_clause  = ", ".join([f"t.{c}=s.{c}" for c in all_cols])
            insert_cols = ", ".join(all_cols)
            insert_vals = ", ".join([f"s.{c}" for c in all_cols])
            spark.sql(f"""
                MERGE INTO {snap_tbl(table_name)} AS t
                USING (SELECT * FROM src_all_vw) AS s
                ON {on}
                WHEN MATCHED THEN UPDATE SET {set_clause}
                WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})
            """)

        complete_snapshot_daily_log(table_name, "I", i_count, start_i, None)
        complete_snapshot_daily_log(table_name, "U", u_count, start_u, None)

        print(f"[{_ts()}] DONE {table_name} | D={d_count}, I={i_count}, U={u_count} | {datetime.now(tz)-start_table}")
    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        complete_snapshot_daily_log(table_name, "U", 0, start_table, msg)
        raise

def apply_with_retry(row):
    table_name = row["TABLE_NAME"]
    key_cols = [c.strip() for c in row["KEY_COLUMN"].split(",")]
    attempts = 0
    while True:
        try:
            apply_snapshot_for_table(table_name, key_cols)
            return
        except Exception:
            attempts += 1
            if attempts > MAX_RETRIES:
                print(f"[{_ts()}] FINAL FAILURE after retries: {table_name}")
                return
            print(f"[{_ts()}] RETRY {attempts}/{MAX_RETRIES} for {table_name} in {SLEEP_BETWEEN_RETRIES}s …")
            time.sleep(SLEEP_BETWEEN_RETRIES)

# === DRIVER QUEUE: failed → pending → already-success ===
def _today_final_logs():
    try:
        return spark.read.table("LOGS.snapshot_final_log").filter(F.col("load_date")==Today_date)
    except:
        return spark.createDataFrame([], snap_log_schema)

def table_has_failure_today(table_name):
    df = _today_final_logs().filter((F.col("table_name")==table_name) & F.col("error_message").isNotNull())
    return not df.limit(1).rdd.isEmpty()

def table_has_all_success_today(table_name):
    df = _today_final_logs().filter((F.col("table_name")==table_name) & F.col("error_message").isNull())
    got = df.select("status").distinct().rdd.map(lambda r: r[0]).collect()
    return set(["I","U","D"]).issubset(set(got))

def process_all_snapshot():
    ensure_snapshot_log_tables_exist()

    print(f"[{_ts()}] Reading TABLES_LIST...")
    tables_df = spark.read.table(TABLES_LIST).select("TABLE_NAME","KEY_COLUMN")
    all_rows = [r.asDict() for r in tables_df.collect()]

    failed, pending, success = [], [], []
    for r in all_rows:
        t = r["TABLE_NAME"]
        if table_has_failure_today(t):
            failed.append(r)
        elif table_has_all_success_today(t):
            success.append(r)
        else:
            pending.append(r)

    queue = failed + pending + success
    if not queue:
        print(f"[{_ts()}] Nothing to process for snapshot apply.")
        return

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(apply_with_retry, r) for r in queue]
        for fut in as_completed(futures):
            fut.result()

    consolidate_snapshot_logs_to_final()

# === RUN ===
process_all_snapshot()
# =================================================================
