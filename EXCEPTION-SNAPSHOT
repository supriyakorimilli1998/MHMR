def apply_snapshot_for_table(table_name, key_cols):
    start_table = datetime.now(tz)
    print(f"[{_ts()}] APPLY SNAPSHOT: {table_name} | Keys = {key_cols}")

    try:
        stg_df = spark.read.table(stg_tbl(table_name))
        stg_today = _staging_today(stg_df)
        status_col = _resolve_status_col(stg_df)

        if stg_today.rdd.isEmpty():
            print(f"[{_ts()}] No data today in {stg_tbl(table_name)}. Logging zeros and exiting.")
            t0 = datetime.now(tz)
            for s in ("D", "I", "U"):
                complete_snapshot_daily_log(table_name, s, 0, t0, None)
            return

        # === Prepare table paths ===
        snap_table = snap_tbl(table_name)
        temp_table = snap_table.replace("_SNAPSHOT", "_TEMP")

        # === Read existing snapshot if exists ===
        try:
            snap_df = spark.read.table(snap_table)
        except AnalysisException:
            print(f"[{_ts()}] Snapshot does not exist. Creating empty snapshot schema.")
            stg_df.limit(0).write.mode("overwrite").saveAsTable(snap_table)
            snap_df = spark.read.table(snap_table)

        all_cols = [c for c in snap_df.columns]

        # === DELETES: keep only rows whose keys are still in today's staging
        start_d = datetime.now(tz)
        stg_keys = stg_today.select(*key_cols).dropDuplicates()
        snap_filtered = snap_df.alias("t").join(
            stg_keys.alias("s"),
            on=[F.col(f"t.{k}") == F.col(f"s.{k}") for k in key_cols],
            how="inner"
        ).select("t.*")
        d_count = snap_df.count() - snap_filtered.count()
        print(f"[{_ts()}] {table_name}: DELETE rows = {d_count}")
        complete_snapshot_daily_log(table_name, "D", d_count, start_d, None)

        # === INSERTS ===
        start_i = datetime.now(tz)
        i_df = stg_today.filter(F.upper(F.col(status_col)) == "I").select(*all_cols)
        i_count = i_df.count()
        print(f"[{_ts()}] {table_name}: INSERT rows = {i_count}")
        complete_snapshot_daily_log(table_name, "I", i_count, start_i, None)

        # === UPDATES ===
        start_u = datetime.now(tz)
        u_df = stg_today.filter(F.upper(F.col(status_col)) == "U").select(*all_cols)
        u_count = u_df.count()
        print(f"[{_ts()}] {table_name}: UPDATE rows = {u_count}")
        complete_snapshot_daily_log(table_name, "U", u_count, start_u, None)

        # === COMBINE: deleted snapshot + inserts + updates ===
        final_df = snap_filtered.unionByName(i_df).unionByName(u_df)
        print(f"[{_ts()}] Writing to TEMP snapshot: {temp_table}")
        final_df.write.mode("overwrite").saveAsTable(temp_table)

        # === ATOMIC SWAP: TEMP â†’ SNAPSHOT ===
        print(f"[{_ts()}] Replacing final snapshot: {snap_table}")
        spark.sql(f"DROP TABLE IF EXISTS {snap_table}")
        spark.sql(f"ALTER TABLE {temp_table} RENAME TO {snap_table}")

        print(f"[{_ts()}] DONE {table_name} | D={d_count}, I={i_count}, U={u_count} | Time: {datetime.now(tz) - start_table}")

    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        complete_snapshot_daily_log(table_name, "U", 0, start_table, msg)
        print(f"[{_ts()}] FAILED {table_name}: {msg}")
        raise
