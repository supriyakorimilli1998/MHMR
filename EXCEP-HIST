# HISTORY  — Logs use I/U/D; empty days logged as 'N' (no-op); success = op_count>0 and no later error

from __future__ import annotations
from datetime import datetime, date
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import traceback
import pytz

from pyspark.sql import SparkSession, functions as F, types as T, Window
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.utils import AnalysisException

# ------------------------
# Spark session & configs
# ------------------------
spark = SparkSession.builder.appName("Apply History Tables - Backfill Aware (Final)").getOrCreate()
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# ------------------------
# Constants (adjust as needed)
# ------------------------
tz = pytz.timezone("America/Chicago")

CATALOG = "MHMR_LAKEHOUSE"
HISTORY_SCHEMA = f"{CATALOG}.HISTORY"
STAGING_SCHEMA = "staging"
TABLES_LIST = "TABLES_LIST"  # columns: TABLE_NAME, KEY_COLUMN (comma-separated keys)

MAX_WORKERS = 4
MAX_RETRIES = 2
SLEEP_BETWEEN_RETRIES = 5

# ------------------------
# Utility helpers
# ------------------------
def _ts() -> str:
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def stg_tbl(table: str) -> str:
    return f"{STAGING_SCHEMA}.stg_{table}"

def hist_tbl(table: str) -> str:
    return f"{HISTORY_SCHEMA}.{table}_HISTORY"

def _local_date_from_ts(col_name: str):
    """Convert a UTC timestamp column to America/Chicago date."""
    return F.to_date(F.from_utc_timestamp(F.col(col_name), "America/Chicago"))

def _resolve_insert_ts_col(df: DataFrame) -> str:
    cols = {c.lower(): c for c in df.columns}
    for c in ("insert_timestamp", "insert_ts"):
        if c in cols:
            return cols[c]
    raise ValueError("Insert timestamp column not found. Expected: insert_timestamp or insert_ts")

def _resolve_status_col(df: DataFrame) -> str:
    for c in df.columns:
        if c.lower() == "status":
            return c
    raise ValueError("STATUS column not found.")

def _staging_on_date(df: DataFrame, d: date) -> DataFrame:
    """Filter staging rows to the given local date derived from insert_ts."""
    ts = _resolve_insert_ts_col(df)
    return df.where(_local_date_from_ts(ts) == F.lit(str(d)).cast("date"))

def _add_flag_column(df: DataFrame, flag_value: str, target_cols: List[str]) -> DataFrame:
    """Ensure FLAG exists and project to target cols in order."""
    work = df
    work = work.withColumn("FLAG", F.lit(flag_value))
    for c in target_cols:
        if c not in work.columns:
            work = work.withColumn(c, F.lit(None))
    return work.select(*target_cols)

# ------------------------
# Generic ensure schema/table helpers
# ------------------------
def _ensure_schema(schema_name: str) -> None:
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")

def _ensure_table(table_fqn: str, schema_struct: T.StructType) -> None:
    """If table doesn't exist, create it empty with the provided schema."""
    _ensure_schema(table_fqn.split('.')[0])
    try:
        spark.read.table(table_fqn).limit(1).count()
    except Exception:
        spark.createDataFrame([], schema_struct).write.mode("overwrite").saveAsTable(table_fqn)

# ------------------------
# Logging (daily reset; final holds history) — STATUSES: I/U/D/E/N
# ------------------------
hist_log_schema = T.StructType([
    T.StructField("load_date", T.DateType()),
    T.StructField("table_name", T.StringType()),
    T.StructField("status", T.StringType()),      # 'I' (insert), 'U' (update), 'D' (inactivation), 'E' (error), 'N' (no-op)
    T.StructField("count", T.LongType()),
    T.StructField("error_message", T.StringType()),
    T.StructField("insert_timestamp", T.TimestampType()),
    T.StructField("start_time", T.TimestampType()),
    T.StructField("end_time", T.TimestampType()),
    T.StructField("load_type", T.StringType()),   # "history"
])

def ensure_history_log_tables_exist() -> None:
    print(f"[{_ts()}] Ensuring history log tables exist ...")
    _ensure_schema("LOGS")
    _ensure_table("LOGS.history_daily_log", hist_log_schema)
    _ensure_table("LOGS.history_final_log", hist_log_schema)

def reset_history_daily_log() -> None:
    ensure_history_log_tables_exist()
    print(f"[{_ts()}] Truncating LOGS.history_daily_log ...")
    spark.sql("TRUNCATE TABLE LOGS.history_daily_log")

def append_history_daily_log(table_name: str, load_date: date, status: str,
                             cnt: int, start_time: datetime,
                             error_message: str | None = None) -> None:
    end_time = datetime.now(tz)
    row_df = spark.createDataFrame(
        [(load_date, table_name, status, int(cnt or 0), error_message,
          end_time, start_time, end_time, "history")],
        schema=hist_log_schema
    )
    print(f"[{_ts()}] LOGS.history_daily_log += table={table_name}, date={load_date}, status={status}, count={cnt}")
    row_df.write.mode("append").saveAsTable("LOGS.history_daily_log")

def consolidate_history_logs_to_final(dedupe: bool = True) -> None:
    print(f"[{_ts()}] Consolidating history daily logs -> final (dedupe={dedupe}) ...")
    try:
        daily = spark.read.table("LOGS.history_daily_log")
    except Exception:
        print(f"[{_ts()}] No LOGS.history_daily_log found; nothing to consolidate.")
        return

    if daily.rdd.isEmpty():
        print(f"[{_ts()}] LOGS.history_daily_log is empty; truncating and returning.")
        spark.sql("TRUNCATE TABLE LOGS.history_daily_log")
        return

    if not dedupe:
        daily.write.mode("append").saveAsTable("LOGS.history_final_log")
        spark.sql("TRUNCATE TABLE LOGS.history_daily_log")
        return

    try:
        final_ = spark.read.table("LOGS.history_final_log")
    except Exception:
        final_ = spark.createDataFrame([], hist_log_schema)

    union_df = final_.unionByName(daily)
    w = Window.partitionBy("load_date", "table_name", "status").orderBy(F.col("end_time").desc_nulls_last())
    dedup = union_df.withColumn("_rn", F.row_number().over(w)).where(F.col("_rn") == 1).drop("_rn")
    dedup.write.mode("overwrite").saveAsTable("LOGS.history_final_log")
    spark.sql("TRUNCATE TABLE LOGS.history_daily_log")
    print(f"[{_ts()}] Consolidation complete.")

# ------------------------
# Success logic (count-aware; errors after ops only)
# ------------------------
def _final_status_rollup(table_name: str) -> DataFrame:
    """
    Returns per-date rollup with:
      op_count      : SUM(count) over I/U/D
      max_op_end    : MAX(end_time) where status in I/U/D
      max_err_end   : MAX(end_time) where status = 'E'
      has_ok        : 1 if op_count>0 and (max_err_end IS NULL or max_err_end < max_op_end)
      has_err       : 1 if max_err_end IS NOT NULL and (max_op_end IS NULL or max_err_end >= max_op_end)
    'N' rows are ignored for success and error.
    """
    try:
        fl = spark.read.table("LOGS.history_final_log").filter(F.col("table_name") == table_name)
    except Exception:
        schema = T.StructType([
            T.StructField("load_date", T.DateType()),
            T.StructField("op_count", T.LongType()),
            T.StructField("max_op_end", T.TimestampType()),
            T.StructField("max_err_end", T.TimestampType()),
            T.StructField("has_ok", T.IntegerType()),
            T.StructField("has_err", T.IntegerType()),
        ])
        return spark.createDataFrame([], schema)

    op = (fl.filter(F.col("status").isin("I", "U", "D"))
            .groupBy("load_date")
            .agg(
                F.sum("count").alias("op_count"),
                F.max("end_time").alias("max_op_end")
            ))

    err = (fl.filter(F.col("status") == "E")
             .groupBy("load_date")
             .agg(F.max("end_time").alias("max_err_end")))

    joined = (op.join(err, on="load_date", how="outer")
                .select(
                    F.col("load_date"),
                    F.coalesce(F.col("op_count"), F.lit(0)).alias("op_count"),
                    F.col("max_op_end"),
                    F.col("max_err_end")
                ))

    has_ok = ( (F.col("op_count") > 0) & (F.col("max_err_end").isNull() | (F.col("max_err_end") < F.col("max_op_end"))) ).cast("int")
    has_err = ( F.col("max_err_end").isNotNull() & (F.col("max_op_end").isNull() | (F.col("max_err_end") >= F.col("max_op_end"))) ).cast("int")

    return joined.select("load_date", "op_count", "max_op_end", "max_err_end", has_ok.alias("has_ok"), has_err.alias("has_err"))

def processed_success_dates_history(table_name: str) -> DataFrame:
    """Dates where has_ok=1."""
    roll = _final_status_rollup(table_name)
    return roll.where(F.col("has_ok") == 1).select("load_date").distinct()

# ------------------------
# Date classification over staging vs FINAL logs (failures-first)
# ------------------------
def _staging_local_dates(stg_df: DataFrame) -> DataFrame:
    ts = _resolve_insert_ts_col(stg_df)
    return stg_df.select(_local_date_from_ts(ts).alias("load_date")).distinct()

def _history_status_by_date(table_name: str) -> DataFrame:
    """Per-date flags computed from FINAL logs rollup."""
    roll = _final_status_rollup(table_name)
    return roll.select(
        "load_date",
        F.col("has_ok").cast("int").alias("has_ok"),
        F.col("has_err").cast("int").alias("has_err")
    )

def classify_history_dates_for_table(table_name: str, stg_df: DataFrame) -> Dict[str, list]:
    """
    For this table, look at all dates present in staging and classify using FINAL logs:
      failed_dates: has_err=1
      success_dates: has_ok=1 (skip)
      unprocessed_dates: neither ok nor err (process)
    """
    sdates = _staging_local_dates(stg_df).alias("s")
    status = _history_status_by_date(table_name).alias("f")
    joined = (sdates.join(status, on="load_date", how="left")
                    .select(
                        F.col("load_date"),
                        F.coalesce(F.col("has_ok"), F.lit(0)).alias("has_ok"),
                        F.coalesce(F.col("has_err"), F.lit(0)).alias("has_err")
                    ))

    failed = (joined.where(F.col("has_err") == 1).orderBy("load_date")
                   .select("load_date").collect())
    success = (joined.where((F.col("has_ok") == 1) & (F.col("has_err") == 0)).orderBy("load_date")
                    .select("load_date").collect())
    unproc = (joined.where((F.col("has_ok") == 0) & (F.col("has_err") == 0)).orderBy("load_date")
                   .select("load_date").collect())

    to_py = lambda rows: [r["load_date"] for r in rows]
    return {
        "failed_dates": to_py(failed),
        "success_dates": to_py(success),
        "unprocessed_dates": to_py(unproc),
    }

# ------------------------
# History table management & key lookups
# ------------------------
def ensure_history_table(table_name: str, stg_like: DataFrame) -> DataFrame:
    """Ensure the history table exists with the same columns as staging plus FLAG (STRING)."""
    hname = hist_tbl(table_name)
    try:
        hdf = spark.read.table(hname)
        if "FLAG" not in [c.upper() for c in hdf.columns]:
            print(f"[{_ts()}] {hname} missing FLAG; seeding empty table with FLAG.")
            empty = stg_like.limit(0).withColumn("FLAG", F.lit(None).cast("string"))
            empty.write.mode("overwrite").saveAsTable(hname)
            hdf = spark.read.table(hname)
        return hdf
    except AnalysisException:
        print(f"[{_ts()}] Creating new history table: {hname}")
        empty = stg_like.limit(0).withColumn("FLAG", F.lit(None).cast("string"))
        empty.write.mode("overwrite").saveAsTable(hname)
        return spark.read.table(hname)

def latest_active_history_rows_for_keys(hdf: DataFrame, key_cols: List[str], keys_df: DataFrame) -> DataFrame:
    """
    From history, pick the latest active ('A') record(s) per key that match keys_df.
    Project to h.* to avoid duplicate key columns.
    """
    order_candidates = [c for c in hdf.columns if c.lower() in ("insert_timestamp", "insert_ts")]
    order_col = order_candidates[0] if order_candidates else None

    join_cond = [F.col(f"h.{k}") == F.col(f"k.{k}") for k in key_cols]

    joined = (
        hdf.alias("h")
           .filter(F.upper(F.col("h.FLAG")) == "A")
           .join(keys_df.alias("k"), on=join_cond, how="inner")
    )

    proj = joined.select(*[F.col(f"h.{c}").alias(c) for c in hdf.columns])

    if order_col:
        w = Window.partitionBy(*[F.col(k) for k in key_cols]).orderBy(F.col(order_col).desc_nulls_last())
    else:
        w = Window.partitionBy(*[F.col(k) for k in key_cols]).orderBy(F.monotonically_increasing_id().desc())

    ranked = proj.withColumn("_rn", F.row_number().over(w))
    pick = ranked.where(F.col("_rn") == 1).drop("_rn")
    return pick

# ------------------------
# History apply per (table, date) — LOG I/U/D/N/E, write FLAG A/I
# ------------------------
def apply_history_for_table_and_date(table_name: str, key_cols: List[str], d: date) -> None:
    """
    - STATUS='D' in staging: clone latest active ('A') history rows for those keys and append with FLAG='I'; log 'D'
    - STATUS='I' in staging: append those rows with FLAG='A'; log 'I'
    - STATUS='U' in staging: append those rows with FLAG='A'; log 'U'
    - Empty day: log 'N' only (no-op)
    """
    start_table_date = datetime.now(tz)
    print(f"[{_ts()}] === APPLY HISTORY: {table_name} @ {d} ===")
    try:
        stg_df = spark.read.table(stg_tbl(table_name))
        status_col = _resolve_status_col(stg_df)
        stg_day = _staging_on_date(stg_df, d)

        if stg_day.rdd.isEmpty():
            print(f"[{_ts()}] No rows on {d} for {stg_tbl(table_name)}.")
            t0 = datetime.now(tz)
            append_history_daily_log(table_name, d, "N", 0, t0, None)  # neutral; does not count as success
            return

        hname = hist_tbl(table_name)
        hist_df = ensure_history_table(table_name, stg_df)
        target_cols = hist_df.columns
        if "FLAG" not in target_cols:
            target_cols = target_cols + ["FLAG"]

        # D (inactivate)
        start_d = datetime.now(tz)
        d_keys = (stg_day.filter(F.upper(F.col(status_col)) == "D")
                         .select(*key_cols).dropDuplicates())
        if d_keys.rdd.isEmpty():
            d_count = 0
            d_out = None
        else:
            latest_active = latest_active_history_rows_for_keys(hist_df, key_cols, d_keys)
            if latest_active.rdd.isEmpty():
                d_count = 0
                d_out = None
            else:
                d_out = _add_flag_column(latest_active, "I", target_cols)
                d_count = d_out.count()
                print(f"[{_ts()}] {table_name} {d} -> D (inactivate) count: {d_count}")
        append_history_daily_log(table_name, d, "D", d_count or 0, start_d, None)

        # I (insert -> active)
        start_i = datetime.now(tz)
        i_src = stg_day.filter(F.upper(F.col(status_col)) == "I")
        if i_src.rdd.isEmpty():
            i_count = 0
            i_out = None
        else:
            i_out = _add_flag_column(i_src, "A", target_cols)
            i_count = i_out.count()
            print(f"[{_ts()}] {table_name} {d} -> I (insert->active) count: {i_count}")
        append_history_daily_log(table_name, d, "I", i_count or 0, start_i, None)

        # U (update -> active)
        start_u = datetime.now(tz)
        u_src = stg_day.filter(F.upper(F.col(status_col)) == "U")
        if u_src.rdd.isEmpty():
            u_count = 0
            u_out = None
        else:
            u_out = _add_flag_column(u_src, "A", target_cols)
            u_count = u_out.count()
            print(f"[{_ts()}] {table_name} {d} -> U (update->active) count: {u_count}")
        append_history_daily_log(table_name, d, "U", u_count or 0, start_u, None)

        if (d_count or 0) == 0 and (i_count or 0) == 0 and (u_count or 0) == 0:
            print(f"[{_ts()}] {table_name} {d} -> No history writes; skipping append.")
            return

        print(f"[{_ts()}] {table_name} {d} -> Appending to {hname}")
        if d_out is not None: d_out.write.mode("append").saveAsTable(hname)
        if i_out is not None: i_out.write.mode("append").saveAsTable(hname)
        if u_out is not None: u_out.write.mode("append").saveAsTable(hname)

        print(f"[{_ts()}] COMPLETE HISTORY {table_name} @ {d}  D={d_count}, I={i_count}, U={u_count}")

    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        append_history_daily_log(table_name, d, "E", 0, start_table_date, msg)
        print(f"[{_ts()}] FAILED HISTORY {table_name} @ {d}\n{msg}")
        raise

# ------------------------
# Orchestration (failures-first, then unprocessed; skip successes)
# ------------------------
def _apply_history_with_retry_for_date(table_name: str, key_cols: List[str], d: date) -> None:
    attempts = 0
    while True:
        try:
            print(f"[{_ts()}] Start HISTORY {table_name} {d} (attempt {attempts + 1})")
            apply_history_for_table_and_date(table_name, key_cols, d)
            return
        except Exception:
            attempts += 1
            if attempts > MAX_RETRIES:
                print(f"[{_ts()}] FINAL FAILURE HISTORY {table_name} {d}")
                return
            print(f"[{_ts()}] RETRY HISTORY in {SLEEP_BETWEEN_RETRIES}s for {table_name} {d}")
            time.sleep(SLEEP_BETWEEN_RETRIES)

def process_all_history() -> None:
    print(f"[{_ts()}] Starting history apply (backfill-aware) ...")
    ensure_history_log_tables_exist()
    reset_history_daily_log()

    tables_df = spark.read.table(TABLES_LIST).select("TABLE_NAME", "KEY_COLUMN")
    all_rows = [r.asDict() for r in tables_df.collect()]
    print(f"[{_ts()}] Tables to scan for history: {len(all_rows)}")

    plan = {}
    any_failed = False
    any_unprocessed = False

    # Build plan: classify dates per table using staging presence vs FINAL logs
    for r in all_rows:
        table = r["TABLE_NAME"]
        key_cols = [c.strip() for c in r["KEY_COLUMN"].split(",") if c.strip()]
        try:
            stg = spark.read.table(stg_tbl(table))
        except AnalysisException:
            print(f"[{_ts()}] Staging missing for HISTORY: {stg_tbl(table)}; skipping")
            continue

        buckets = classify_history_dates_for_table(table, stg)
        plan[table] = {"keys": key_cols, **buckets}

        if buckets["failed_dates"]:
            any_failed = True
        if buckets["unprocessed_dates"]:
            any_unprocessed = True

        print(f"[{_ts()}] Plan {table}: "
              f"failed={buckets['failed_dates']}, "
              f"success={buckets['success_dates']}, "
              f"unprocessed={buckets['unprocessed_dates']}")

    if not any_failed and not any_unprocessed:
        print(f"[{_ts()}] All tables/dates already processed successfully. Nothing to do.")
        consolidate_history_logs_to_final(dedupe=True)
        return

    mode = "failed_only" if any_failed else "unprocessed_only"
    print(f"[{_ts()}] Execution mode: {mode}")

    def work_for_table(table: str, key_cols: List[str], dates: List[date]):
        if not dates: return
        print(f"[{_ts()}] HISTORY processing -> {table}: {', '.join(map(str, dates))}")
        for d in dates:
            _apply_history_with_retry_for_date(table, key_cols, d)

    # Parallelize across tables; keep per-table dates serial
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = []
        for table, info in plan.items():
            dates = info["failed_dates"] if mode == "failed_only" else info["unprocessed_dates"]
            futures.append(ex.submit(work_for_table, table, info["keys"], dates))
        for f in as_completed(futures):
            f.result()

    consolidate_history_logs_to_final(dedupe=True)
    print(f"[{_ts()}] All history processing complete.")

# Entry point
process_all_history()
