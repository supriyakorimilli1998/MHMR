# =====================================================================
# FINAL FABRIC SCRIPT (PRINT VERSION)
# - Migration utilities
# - Migration orchestrator (scales to 500+ tables)
# - History processor
# =====================================================================

from pyspark.sql import SparkSession, functions as F, types as T, Window
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.utils import AnalysisException
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, traceback, os

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("History Pipeline with Migration").getOrCreate()
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

tz = pytz.timezone("America/Chicago")
TABLES_LIST = "TABLES_LIST"
DAILY_LOG = "LOGS.history_daily_log"
FINAL_LOG = "LOGS.history_final_log"

# === Helpers ===
def _ts(): return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
def p(msg): print(f"[{_ts()}] {msg}")
def stg_tbl(t): return f"staging.stg_{t}"

def _table_exists(name: str) -> bool:
    try:
        spark.read.table(name).limit(1).count()
        return True
    except Exception:
        return False

def _ensure_schema(schema: str):
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema}")

# ==========================================================================================
# 1. MIGRATION UTILITIES (print-only)
# ==========================================================================================

def _project_for_write(df: DataFrame) -> DataFrame:
    cols = {c.lower(): c for c in df.columns}
    ts_col = None
    for c in ("insert_timestamp", "insert_ts"):
        if c in cols:
            ts_col = cols[c]
            break
    if not ts_col:
        raise ValueError("Insert timestamp column not found.")
    if "STG_LOAD_DATE" not in df.columns:
        df = df.withColumn(
            "STG_LOAD_DATE",
            F.to_date(F.from_utc_timestamp(F.col(ts_col), "America/Chicago"))
        )
    cols_out = [c for c in df.columns if c != "STG_LOAD_DATE"] + ["STG_LOAD_DATE"]
    return df.select(*cols_out)

def migrate_staging_to_partition(table_name: str) -> None:
    src = stg_tbl(table_name)
    tmp = f"{src}_TMP_PART"
    bak = f"{src}_BAK_{int(datetime.now().timestamp())}"

    p(f"[MIGRATE] Starting {src}")
    if not _table_exists(src):
        p(f"[MIGRATE] Source table missing; skipping")
        return

    try:
        desc = spark.sql(f"DESCRIBE DETAIL {src}").collect()[0].asDict(recursive=True)
        parts = [c.upper() for c in (desc.get("partitionColumns", []) or [])]
        if "STG_LOAD_DATE" in parts:
            p(f"[MIGRATE] Already partitioned by STG_LOAD_DATE; skipping.")
            return
    except Exception as e:
        p(f"[MIGRATE] WARN: cannot describe detail ({e})")

    df = spark.read.table(src)
    dfw = _project_for_write(df)

    p(f"[MIGRATE] Writing temp {tmp}")
    dfw.write.mode("overwrite").partitionBy("STG_LOAD_DATE").saveAsTable(tmp)

    p(f"[MIGRATE] Swapping: {src} -> {bak}, {tmp} -> {src}")
    spark.sql(f"ALTER TABLE {src} RENAME TO {bak}")
    spark.sql(f"ALTER TABLE {tmp} RENAME TO {src}")

    p(f"[MIGRATE] DONE {src}. Backup: {bak}")

def check_staging_partition(table_name: str):
    src = stg_tbl(table_name)
    p(f"[CHECK] {src}")
    if not _table_exists(src):
        p("[CHECK] Table missing.")
        return
    try:
        d = spark.sql(f"DESCRIBE DETAIL {src}").collect()[0].asDict(recursive=True)
        p(f"[CHECK] partitionColumns={d.get('partitionColumns', [])}")
    except Exception as e:
        p(f"[CHECK] describe failed: {e}")
    df = spark.read.table(src)
    if "STG_LOAD_DATE" in df.columns:
        df.groupBy("STG_LOAD_DATE").count().orderBy("STG_LOAD_DATE").show(20, False)

# ==========================================================================================
# 2. MIGRATION ORCHESTRATOR (scales to 500+ tables)
# ==========================================================================================

MIG_MAX_CONCURRENT = 6
MIG_MAX_TABLES_PER_RUN = 50
MIG_DRY_RUN = False

def _ensure_mig_status_table():
    _ensure_schema("LOGS")
    if not _table_exists("LOGS.staging_migration_status"):
        schema = T.StructType([
            T.StructField("table_name", T.StringType()),
            T.StructField("state", T.StringType()),
            T.StructField("last_message", T.StringType()),
            T.StructField("updated_ts", T.TimestampType()),
        ])
        spark.createDataFrame([], schema).write.mode("overwrite").saveAsTable("LOGS.staging_migration_status")

def _update_mig_status(table_name, state, message):
    row = [(table_name, state, message, datetime.now(tz))]
    df = spark.createDataFrame(row, "table_name string, state string, last_message string, updated_ts timestamp")
    df.write.mode("append").saveAsTable("LOGS.staging_migration_status")

def _latest_mig_status():
    try:
        df = spark.read.table("LOGS.staging_migration_status")
    except Exception:
        return spark.createDataFrame([], "table_name string, state string, last_message string, updated_ts timestamp")
    w = Window.partitionBy("table_name").orderBy(F.col("updated_ts").desc_nulls_last())
    return df.withColumn("_rn", F.row_number().over(w)).where(F.col("_rn")==1).drop("_rn")

def _plan_migration_batch():
    _ensure_mig_status_table()
    latest = _latest_mig_status().select("table_name","state")
    try:
        names = [r["TABLE_NAME"] for r in spark.read.table(TABLES_LIST).select("TABLE_NAME").collect()]
    except AnalysisException:
        p("[MIG-ORCH] ERROR: TABLES_LIST missing.")
        return []

    todo = []
    for t in names:
        fqn = stg_tbl(t)
        if not _table_exists(fqn):
            _update_mig_status(t, "SKIPPED", "staging table missing")
            continue
        if _latest_mig_status().filter(F.col("table_name")==t).filter(F.col("state")=="DONE").count()>0:
            continue
        todo.append(t)

    if len(todo) > MIG_MAX_TABLES_PER_RUN:
        todo = todo[:MIG_MAX_TABLES_PER_RUN]
    return todo

def run_migration_orchestrator():
    p("[MIG-ORCH] === START ===")
    batch = _plan_migration_batch()
    if not batch:
        p("[MIG-ORCH] Nothing to migrate.")
        return
    if MIG_DRY_RUN:
        for t in batch: p(f"[MIG-ORCH] would migrate {stg_tbl(t)}")
        return
    def _migrate_one(t):
        try:
            migrate_staging_to_partition(t)
            _update_mig_status(t, "DONE", "ok")
            return (t,"DONE")
        except Exception as e:
            _update_mig_status(t, "FAILED", str(e))
            return (t,"FAILED")
    results=[]
    with ThreadPoolExecutor(max_workers=MIG_MAX_CONCURRENT) as ex:
        futures=[ex.submit(_migrate_one,t) for t in batch]
        for f in as_completed(futures): results.append(f.result())
    done=[t for t,s in results if s=="DONE"]
    fail=[t for t,s in results if s=="FAILED"]
    p(f"[MIG-ORCH] Done={len(done)}, Failed={len(fail)}")
    p("[MIG-ORCH] === END ===")

# ==========================================================================================
# 3. HISTORY PROCESSOR (your daily recurring job)
# ==========================================================================================

def _ensure_log_tables():
    _ensure_schema("LOGS")
    schema = "table_name string, process_date date, status string, inserted_count int, updated_count int, error string, log_ts timestamp"
    for tbl in [DAILY_LOG, FINAL_LOG]:
        if not _table_exists(tbl):
            spark.createDataFrame([], schema).write.mode("overwrite").saveAsTable(tbl)

def process_table_for_date(table_name: str, process_date):
    # stub: replace with your actual I/U logic
    inserted, updated = 0, 0
    p(f"[HIST] Processing {table_name} for {process_date}")
    df = spark.read.table(stg_tbl(table_name)).filter(F.col("STG_LOAD_DATE")==F.lit(process_date))
    count = df.count()
    inserted, updated = count, 0
    return inserted, updated

def process_all_history():
    _ensure_log_tables()
    try:
        tbls = [r["TABLE_NAME"] for r in spark.read.table(TABLES_LIST).select("TABLE_NAME").collect()]
    except AnalysisException:
        p("[HIST] ERROR: TABLES_LIST missing")
        return
    for t in tbls:
        if not _table_exists(stg_tbl(t)):
            p(f"[HIST] skip {t} (no staging)")
            continue
        dates = [r[0] for r in spark.read.table(stg_tbl(t)).select("STG_LOAD_DATE").distinct().collect()]
        for d in dates:
            try:
                ins, upd = process_table_for_date(t,d)
                row=[(t,d,"SUCCESS",ins,upd,None,datetime.now(tz))]
                spark.createDataFrame(row,"table_name string, process_date date, status string, inserted_count int, updated_count int, error string, log_ts timestamp").write.mode("append").saveAsTable(DAILY_LOG)
                spark.createDataFrame(row,"table_name string, process_date date, status string, inserted_count int, updated_count int, error string, log_ts timestamp").write.mode("append").saveAsTable(FINAL_LOG)
            except Exception as e:
                row=[(t,d,"FAILED",0,0,str(e),datetime.now(tz))]
                spark.createDataFrame(row,"table_name string, process_date date, status string, inserted_count int, updated_count int, error string, log_ts timestamp").write.mode("append").saveAsTable(DAILY_LOG)
                spark.createDataFrame(row,"table_name string, process_date date, status string, inserted_count int, updated_count int, error string, log_ts timestamp").write.mode("append").saveAsTable(FINAL_LOG)

# ==========================================================================================
# 4. ENTRYPOINTS
# ==========================================================================================

# --- One-time migration until all staging tables are partitioned ---
# run_migration_orchestrator()

# --- Daily recurring job ---
# process_all_history()
