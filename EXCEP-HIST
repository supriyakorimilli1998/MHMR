# HISTORY — CST everywhere; logic per your rules
# I in staging  -> append to HISTORY with FLAG='A'
# U in staging  -> append to HISTORY with FLAG='A'
# D in staging  -> lookup active rows for keys; append those rows with FLAG='I' (inactivate). Do NOT append D staging rows.
# Success = sum(I/U/D counts) > 0 AND no later error; empty day logs 'N' only

from __future__ import annotations
from datetime import datetime, date
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import traceback
import pytz

from pyspark.sql import SparkSession, functions as F, types as T, Window
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.utils import AnalysisException

# ------------------------
# Spark session & configs
# ------------------------
spark = SparkSession.builder.appName("Apply History Tables - Backfill Aware (Final)").getOrCreate()

# Store & display timestamps in CST/CDT
spark.conf.set("spark.sql.session.timeZone", "America/Chicago")

spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# ------------------------
# Constants (adjust as needed)
# ------------------------
tz = pytz.timezone("America/Chicago")

CATALOG = "MHMR_LAKEHOUSE"
HISTORY_SCHEMA = f"{CATALOG}.HISTORY"
STAGING_SCHEMA = "staging"
TABLES_LIST = "TABLES_LIST"  # columns: TABLE_NAME, KEY_COLUMN (comma-separated keys)

# Set this based on how staging stores its timestamps
STAGING_TIMESTAMPS_ARE_UTC = True

MAX_WORKERS = 4
MAX_RETRIES = 2
SLEEP_BETWEEN_RETRIES = 5

# ------------------------
# Utility helpers
# ------------------------
def _ts() -> str:
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def stg_tbl(table: str) -> str:
    return f"{STAGING_SCHEMA}.stg_{table}"

def hist_tbl(table: str) -> str:
    return f"{HISTORY_SCHEMA}.{table}_HISTORY"

def _resolve_insert_ts_col(df: DataFrame) -> str:
    cols = {c.lower(): c for c in df.columns}
    for c in ("insert_timestamp", "insert_ts"):
        if c in cols:
            return cols[c]
    raise ValueError("Insert timestamp column not found. Expected: insert_timestamp or insert_ts")

def _resolve_status_col(df: DataFrame) -> str:
    for c in df.columns:
        if c.lower() == "status":
            return c
    raise ValueError("STATUS column not found.")

def _to_cst_date_from_ts(col_name: str):
    """Return CST/CDT date from a timestamp column, respecting STAGING_TIMESTAMPS_ARE_UTC."""
    if STAGING_TIMESTAMPS_ARE_UTC:
        return F.to_date(F.from_utc_timestamp(F.col(col_name), "America/Chicago"))
    else:
        return F.to_date(F.col(col_name))

def _staging_on_date(df: DataFrame, d: date) -> DataFrame:
    ts = _resolve_insert_ts_col(df)
    return df.where(_to_cst_date_from_ts(ts) == F.lit(str(d)).cast("date"))

def _localize_staging_ts(df: DataFrame) -> DataFrame:
    """Make sure the staging timestamp column is CST when we write it to HISTORY."""
    ts_col = _resolve_insert_ts_col(df)
    if STAGING_TIMESTAMPS_ARE_UTC:
        return df.withColumn(ts_col, F.from_utc_timestamp(F.col(ts_col), "America/Chicago"))
    else:
        return df

def _add_flag_column(df: DataFrame, flag_value: str, target_cols: List[str]) -> DataFrame:
    """Ensure FLAG exists and project to target cols in order."""
    work = df.withColumn("FLAG", F.lit(flag_value))
    for c in target_cols:
        if c not in work.columns:
            work = work.withColumn(c, F.lit(None))
    return work.select(*target_cols)

# ------------------------
# Ensure schema/table helpers
# ------------------------
def _ensure_schema(schema_name: str) -> None:
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")

def _ensure_table(table_fqn: str, schema_struct: T.StructType) -> None:
    _ensure_schema(table_fqn.split('.')[0])
    try:
        spark.read.table(table_fqn).limit(1).count()
    except Exception:
        spark.createDataFrame([], schema_struct).write.mode("overwrite").saveAsTable(table_fqn)

# ------------------------
# Logging (CST) — STATUSES: I/U/D/E/N
# ------------------------
hist_log_schema = T.StructType([
    T.StructField("load_date", T.DateType()),
    T.StructField("table_name", T.StringType()),
    T.StructField("status", T.StringType()),      # 'I','U','D' (from staging/inactivation), 'E' (error), 'N' (no-op)
    T.StructField("count", T.LongType()),
    T.StructField("error_message", T.StringType()),
    T.StructField("insert_timestamp", T.TimestampType()),  # CST/CDT
    T.StructField("start_time", T.TimestampType()),        # CST/CDT
    T.StructField("end_time", T.TimestampType()),          # CST/CDT
    T.StructField("load_type", T.StringType()),            # "history"
])

def ensure_history_log_tables_exist() -> None:
    print(f"[{_ts()}] Ensuring history log tables exist ...")
    _ensure_schema("LOGS")
    _ensure_table("LOGS.history_daily_log", hist_log_schema)
    _ensure_table("LOGS.history_final_log", hist_log_schema)

def reset_history_daily_log() -> None:
    ensure_history_log_tables_exist()
    print(f"[{_ts()}] Truncating LOGS.history_daily_log ...")
    spark.sql("TRUNCATE TABLE LOGS.history_daily_log")

def _now_ts():
    return F.current_timestamp()  # With session TZ set to CST/CDT

def append_history_daily_log(table_name: str, load_date: date, status: str,
                             cnt: int, start_time_py: datetime | None = None,
                             error_message: str | None = None) -> None:
    start_ts = F.lit(start_time_py).cast("timestamp") if start_time_py else _now_ts()
    row_df = (spark.range(1).select(
        F.lit(load_date).cast("date").alias("load_date"),
        F.lit(table_name).alias("table_name"),
        F.lit(status).alias("status"),
        F.lit(int(cnt or 0)).cast("long").alias("count"),
        F.lit(error_message).cast("string").alias("error_message"),
        _now_ts().alias("insert_timestamp"),
        start_ts.alias("start_time"),
        _now_ts().alias("end_time"),
        F.lit("history").alias("load_type"),
    ))
    print(f"[{_ts()}] LOGS.history_daily_log += table={table_name}, date={load_date}, status={status}, count={cnt}")
    row_df.write.mode("append").saveAsTable("LOGS.history_daily_log")

def consolidate_history_logs_to_final(dedupe: bool = True) -> None:
    print(f"[{_ts()}] Consolidating history daily logs -> final (dedupe={dedupe}) ...")
    try:
        daily = spark.read.table("LOGS.history_daily_log")
    except Exception:
        print(f"[{_ts()}] No LOGS.history_daily_log found; nothing to consolidate.")
        return

    if daily.rdd.isEmpty():
        print(f"[{_ts()}] LOGS.history_daily_log is empty; truncating and returning.")
        spark.sql("TRUNCATE TABLE LOGS.history_daily_log")
        return

    if not dedupe:
        daily.write.mode("append").saveAsTable("LOGS.history_final_log")
        spark.sql("TRUNCATE TABLE LOGS.history_daily_log")
        return

    try:
        final_ = spark.read.table("LOGS.history_final_log")
    except Exception:
        final_ = spark.createDataFrame([], hist_log_schema)

    union_df = final_.unionByName(daily)
    w = Window.partitionBy("load_date", "table_name", "status").orderBy(F.col("end_time").desc_nulls_last())
    dedup = union_df.withColumn("_rn", F.row_number().over(w)).where(F.col("_rn") == 1).drop("_rn")
    dedup.write.mode("overwrite").saveAsTable("LOGS.history_final_log")
    spark.sql("TRUNCATE TABLE LOGS.history_daily_log")
    print(f"[{_ts()}] Consolidation complete.")

# ------------------------
# Success logic (count-aware; errors after ops only)
# ------------------------
def _final_status_rollup(table_name: str) -> DataFrame:
    """
    Per date:
      op_count    : SUM(count) over I/U/D
      max_op_end  : latest end_time over I/U/D
      max_err_end : latest end_time over E
      has_ok      : op_count>0 AND (no error after latest op)
      has_err     : error present AND (no op after that error)
    """
    try:
        fl = spark.read.table("LOGS.history_final_log").filter(F.col("table_name") == table_name)
    except Exception:
        schema = T.StructType([
            T.StructField("load_date", T.DateType()),
            T.StructField("op_count", T.LongType()),
            T.StructField("max_op_end", T.TimestampType()),
            T.StructField("max_err_end", T.TimestampType()),
            T.StructField("has_ok", T.IntegerType()),
            T.StructField("has_err", T.IntegerType()),
        ])
        return spark.createDataFrame([], schema)

    op = (fl.filter(F.col("status").isin("I", "U", "D"))
            .groupBy("load_date")
            .agg(F.sum("count").alias("op_count"),
                 F.max("end_time").alias("max_op_end")))
    err = (fl.filter(F.col("status") == "E")
            .groupBy("load_date")
            .agg(F.max("end_time").alias("max_err_end")))

    joined = (op.join(err, on="load_date", how="outer")
                .select(F.col("load_date"),
                        F.coalesce(F.col("op_count"), F.lit(0)).alias("op_count"),
                        F.col("max_op_end"),
                        F.col("max_err_end")))
    has_ok = ((F.col("op_count") > 0) & (F.col("max_err_end").isNull() | (F.col("max_err_end") < F.col("max_op_end")))).cast("int")
    has_err = (F.col("max_err_end").isNotNull() & (F.col("max_op_end").isNull() | (F.col("max_err_end") >= F.col("max_op_end")))).cast("int")
    return joined.select("load_date", "op_count", "max_op_end", "max_err_end",
                         has_ok.alias("has_ok"), has_err.alias("has_err"))

def processed_success_dates_history(table_name: str) -> DataFrame:
    return _final_status_rollup(table_name).where(F.col("has_ok") == 1).select("load_date").distinct()

# ------------------------
# Date classification (failures-first)
# ------------------------
def _staging_local_dates(stg_df: DataFrame) -> DataFrame:
    ts = _resolve_insert_ts_col(stg_df)
    return stg_df.select(_to_cst_date_from_ts(ts).alias("load_date")).distinct()

def _history_status_by_date(table_name: str) -> DataFrame:
    roll = _final_status_rollup(table_name)
    return roll.select("load_date",
                       F.col("has_ok").cast("int").alias("has_ok"),
                       F.col("has_err").cast("int").alias("has_err"))

def classify_history_dates_for_table(table_name: str, stg_df: DataFrame) -> Dict[str, list]:
    sdates = _staging_local_dates(stg_df).alias("s")
    status = _history_status_by_date(table_name).alias("f")
    joined = (sdates.join(status, on="load_date", how="left")
                    .select(F.col("load_date"),
                            F.coalesce(F.col("has_ok"), F.lit(0)).alias("has_ok"),
                            F.coalesce(F.col("has_err"), F.lit(0)).alias("has_err")))
    failed = (joined.where(F.col("has_err") == 1).orderBy("load_date").select("load_date").collect())
    success = (joined.where((F.col("has_ok") == 1) & (F.col("has_err") == 0)).orderBy("load_date").select("load_date").collect())
    unproc = (joined.where((F.col("has_ok") == 0) & (F.col("has_err") == 0)).orderBy("load_date").select("load_date").collect())
    to_py = lambda rows: [r["load_date"] for r in rows]
    return {"failed_dates": to_py(failed), "success_dates": to_py(success), "unprocessed_dates": to_py(unproc)}

# ------------------------
# History table management & key lookups
# ------------------------
def ensure_history_table(table_name: str, stg_like: DataFrame) -> DataFrame:
    """Ensure the history table exists with the same columns as staging plus FLAG (STRING)."""
    hname = hist_tbl(table_name)
    try:
        hdf = spark.read.table(hname)
        if "FLAG" not in [c.upper() for c in hdf.columns]:
            print(f"[{_ts()}] {hname} missing FLAG; seeding empty table with FLAG.")
            empty = stg_like.limit(0).withColumn("FLAG", F.lit(None).cast("string"))
            empty.write.mode("overwrite").saveAsTable(hname)
            hdf = spark.read.table(hname)
        return hdf
    except AnalysisException:
        print(f"[{_ts()}] Creating new history table: {hname}")
        empty = stg_like.limit(0).withColumn("FLAG", F.lit(None).cast("string"))
        empty.write.mode("overwrite").saveAsTable(hname)
        return spark.read.table(hname)

def latest_active_history_rows_for_keys(hdf: DataFrame, key_cols: List[str], keys_df: DataFrame) -> DataFrame:
    """
    Get the latest active ('A') row per key.
    Join on qualified cols; then project h.* to avoid duplicate key columns (prevents ambiguous refs).
    """
    order_candidates = [c for c in hdf.columns if c.lower() in ("insert_timestamp", "insert_ts")]
    order_col = order_candidates[0] if order_candidates else None
    join_cond = [F.col(f"h.{k}") == F.col(f"k.{k}") for k in key_cols]

    joined = (hdf.alias("h")
                .filter(F.upper(F.col("h.FLAG")) == "A")
                .join(keys_df.alias("k"), on=join_cond, how="inner"))

    proj = joined.select(*[F.col(f"h.{c}").alias(c) for c in hdf.columns])

    if order_col:
        w = Window.partitionBy(*[F.col(k) for k in key_cols]).orderBy(F.col(order_col).desc_nulls_last())
    else:
        w = Window.partitionBy(*[F.col(k) for k in key_cols]).orderBy(F.monotonically_increasing_id().desc())

    return proj.withColumn("_rn", F.row_number().over(w)).where(F.col("_rn") == 1).drop("_rn")

# ------------------------
# History apply — per (table, date)
# ------------------------
def apply_history_for_table_and_date(table_name: str, key_cols: List[str], d: date) -> None:
    """
    Your rules:
      I: append staging rows with FLAG='A'
      U: append staging rows with FLAG='A'
      D: append inactive clone(s) of matching active history rows (FLAG='I'); do NOT append staging D rows
      Empty day: log 'N' only
    """
    start_table_date = datetime.now(tz)
    print(f"[{_ts()}] === APPLY HISTORY: {table_name} @ {d} ===")
    try:
        stg_df = spark.read.table(stg_tbl(table_name))
        status_col = _resolve_status_col(stg_df)
        stg_day = _staging_on_date(stg_df, d)

        if stg_day.rdd.isEmpty():
            print(f"[{_ts()}] No rows on {d} for {stg_tbl(table_name)}.")
            append_history_daily_log(table_name, d, "N", 0, start_table_date, None)
            return

        hname = hist_tbl(table_name)
        hist_df = ensure_history_table(table_name, stg_df)
        target_cols = hist_df.columns
        if "FLAG" not in target_cols:
            target_cols = target_cols + ["FLAG"]

        # Partition staging by op and localize timestamps to CST
        d_src = _localize_staging_ts(stg_day.filter(F.upper(F.col(status_col)) == "D"))
        i_src = _localize_staging_ts(stg_day.filter(F.upper(F.col(status_col)) == "I"))
        u_src = _localize_staging_ts(stg_day.filter(F.upper(F.col(status_col)) == "U"))

        # --------- D (inactivate actives by keys) ----------
        start_d = datetime.now(tz)
        d_inact_out = None
        d_count_inactivated = 0
        if not d_src.rdd.isEmpty():
            d_keys = d_src.select(*key_cols).dropDuplicates()
            if not d_keys.rdd.isEmpty():
                latest_active_d = latest_active_history_rows_for_keys(hist_df, key_cols, d_keys)
                if not latest_active_d.rdd.isEmpty():
                    d_inact_out = _add_flag_column(latest_active_d, "I", target_cols)
                    d_count_inactivated = d_inact_out.count()
        append_history_daily_log(table_name, d, "D", d_count_inactivated, start_d, None)

        # --------- I (append as active) ----------
        start_i = datetime.now(tz)
        i_out = None
        i_count = 0
        if not i_src.rdd.isEmpty():
            i_out = _add_flag_column(i_src, "A", target_cols)
            i_count = i_src.count()
        append_history_daily_log(table_name, d, "I", i_count, start_i, None)

        # --------- U (append as active) ----------
        start_u = datetime.now(tz)
        u_out = None
        u_count = 0
        if not u_src.rdd.isEmpty():
            u_out = _add_flag_column(u_src, "A", target_cols)
            u_count = u_src.count()
        append_history_daily_log(table_name, d, "U", u_count, start_u, None)

        # If nothing to write, stop here
        if all(x is None for x in [d_inact_out, i_out, u_out]):
            print(f"[{_ts()}] {table_name} {d} -> No history writes; skipping append.")
            return

        print(f"[{_ts()}] {table_name} {d} -> Appending to {hname}")
        for out in [d_inact_out, i_out, u_out]:
            if out is not None:
                out.write.mode("append").saveAsTable(hname)

        print(f"[{_ts()}] COMPLETE HISTORY {table_name} @ {d}  D(inactivated)={d_count_inactivated}, I(stg)={i_count}, U(stg)={u_count}")

    except Exception as e:
        msg = f"{type(e).__name__}: {str(e)}"
        traceback.print_exc()
        append_history_daily_log(table_name, d, "E", 0, start_table_date, msg)
        print(f"[{_ts()}] FAILED HISTORY {table_name} @ {d}\n{msg}")
        raise

# ------------------------
# Orchestration (failures-first, then unprocessed; skip successes)
# ------------------------
def _apply_history_with_retry_for_date(table_name: str, key_cols: List[str], d: date) -> None:
    attempts = 0
    while True:
        try:
            print(f"[{_ts()}] Start HISTORY {table_name} {d} (attempt {attempts + 1})")
            apply_history_for_table_and_date(table_name, key_cols, d)
            return
        except Exception:
            attempts += 1
            if attempts > MAX_RETRIES:
                print(f"[{_ts()}] FINAL FAILURE HISTORY {table_name} {d}")
                return
            print(f"[{_ts()}] RETRY HISTORY in {SLEEP_BETWEEN_RETRIES}s for {table_name} {d}")
            time.sleep(SLEEP_BETWEEN_RETRIES)

def process_all_history() -> None:
    print(f"[{_ts()}] Starting history apply (backfill-aware) ...")
    ensure_history_log_tables_exist()
    reset_history_daily_log()

    tables_df = spark.read.table(TABLES_LIST).select("TABLE_NAME", "KEY_COLUMN")
    all_rows = [r.asDict() for r in tables_df.collect()]
    print(f"[{_ts()}] Tables to scan for history: {len(all_rows)}")

    plan = {}
    any_failed = False
    any_unprocessed = False

    for r in all_rows:
        table = r["TABLE_NAME"]
        key_cols = [c.strip() for c in r["KEY_COLUMN"].split(",") if c.strip()]
        try:
            stg = spark.read.table(stg_tbl(table))
        except AnalysisException:
            print(f"[{_ts()}] Staging missing for HISTORY: {stg_tbl(table)}; skipping")
            continue

        buckets = classify_history_dates_for_table(table, stg)
        plan[table] = {"keys": key_cols, **buckets}
        if buckets["failed_dates"]: any_failed = True
        if buckets["unprocessed_dates"]: any_unprocessed = True

        print(f"[{_ts()}] Plan {table}: failed={buckets['failed_dates']}, success={buckets['success_dates']}, unprocessed={buckets['unprocessed_dates']}")

    if not any_failed and not any_unprocessed:
        print(f"[{_ts()}] All tables/dates already processed successfully. Nothing to do.")
        consolidate_history_logs_to_final(dedupe=True)
        return

    mode = "failed_only" if any_failed else "unprocessed_only"
    print(f"[{_ts()}] Execution mode: {mode}")

    def work_for_table(table: str, key_cols: List[str], dates: List[date]):
        if not dates: return
        print(f"[{_ts()}] HISTORY processing -> {table}: {', '.join(map(str, dates))}")
        for d in dates:
            _apply_history_with_retry_for_date(table, key_cols, d)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = []
        for table, info in plan.items():
            dates = info["failed_dates"] if mode == "failed_only" else info["unprocessed_dates"]
            futures.append(ex.submit(work_for_table, table, info["keys"], dates))
        for f in as_completed(futures):
            f.result()

    consolidate_history_logs_to_final(dedupe=True)
    print(f"[{_ts()}] All history processing complete.")

# Entry point
process_all_history()
