# --- IMPORTS ---loading as  a table format
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws, when, lit, substring, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import pytz
import os
import uuid
import traceback

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process History Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# --- CONFIG ---
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()
lock_file = "/tmp/update_log.lock"

# --- UTILITY FUNCTIONS ---
def release_lock():
    if os.path.exists(lock_file):
        os.remove(lock_file)

def add_hash_column(df):
    exclude_cols = {"ROW_HASH", "insert_timestamp", "UTCTimestamp", "STATUS", "flag"}
    cleaned_cols = [
        when(col(f.name).isNull(), lit(" ")).otherwise(col(f.name).cast("string")).alias(f.name)
        for f in df.schema.fields if f.name not in exclude_cols
    ]
    temp_df = df.select(*cleaned_cols)
    row_hash_col = sha2(concat_ws("||", *temp_df.columns), 256).alias("ROW_HASH")
    return df.withColumn("ROW_HASH", row_hash_col)

def safe_read_table(table_name):
    try:
        return spark.read.table(table_name)
    except Exception as e:
        print(f"FUNCTION safe_read_table:  Failed to read table: {table_name}")
        traceback.print_exc()
        return None

def update_log(table_name, status, utc_timestamp_str, start_time, end_time, count="0", error_message=""):
    print(f"FUNCTION update_log: [LOG] Updating log for table: {table_name}, status: {status}, UTC: {utc_timestamp_str}")

    run_date = datetime.now(pytz.timezone("America/Chicago")).date()

    new_row = [(str(table_name),
                run_date,
                str(utc_timestamp_str),
                str(status), 
                str(start_time.isoformat()), 
                str(end_time.isoformat()), 
                str(count), 
                str(error_message))
                ]

    print(f"FUNCTION update_log:LOG_NEW_ROW: {new_row}")

    df_logs = spark.read.table("daily_history_logs")
    #df_logs.printSchema()

    schema = spark.read.table("daily_history_logs").schema
    new_df = spark.createDataFrame(new_row, schema)

    def acquire_lock(timeout=30):
        import time
        start = time.time()
        while os.path.exists(lock_file):
            if time.time() - start > timeout:
                return False
            time.sleep(0.5)
        with open(lock_file, 'w') as f:
            f.write("locked")
        return True

    if not acquire_lock():
        print(" FUNCTION update_log: [LOCK] Could not acquire lock. Skipping log update.")
        return

    try:
        try:
            existing_df = spark.read.table("daily_history_logs")
            filtered_df = existing_df.filter(~(
                (col("table_name") == table_name) &
                (col("load_date") == run_date) &
                (col("UTCTimestamp") == utc_timestamp_str) &
                (col("status") == status)
            ))
            final_df = filtered_df.unionByName(new_df)
        except:
            final_df = new_df

        final_df.write.mode("overwrite").saveAsTable("daily_history_logs")
        print("FUNCTION acquire_lock: [LOG] Log updated successfully.")
        
    except Exception:
        print("FUNCTION update_log:[ERROR] Exception in update_log()")
        traceback.print_exc()
    finally:
        release_lock()

def get_unprocessed_utcs_for_table(table_name, log_df, mode="ALL"):

    print(f" FUNCTION get_unprocessed_utcs_for_table: [DEBUG] Getting unprocessed UTCs for: {table_name} | Mode: {mode}")

    try:
        insert_df = safe_read_table(f"{table_name}_INSERT")
        if insert_df is None:
            print(f"FUNCTION get_unprocessed_utcs_for_table:[INFO] INSERT table not found for {table_name}")
            return []

        utc_dates = insert_df.select(substring(col("UTCTimestamp"), 1, 10)) \
                             .distinct().rdd.flatMap(lambda x: x).collect()

        if mode == "FAILURE":
            failed_utcs = log_df.filter((col("table_name") == table_name) & (col("status") == "FAILURE")) \
                                .select("UTCTimestamp").rdd.flatMap(lambda x: x).map(lambda d: d[:10]).distinct().collect()
            print(f"FUNCTION get_unprocessed_utcs_for_table: [DEBUG] Failure UTCs: {failed_utcs}")
            return [utc for utc in utc_dates if utc in failed_utcs]
        else:
            logged_utcs = log_df.filter(col("table_name") == table_name) \
                                .select("UTCTimestamp").rdd.flatMap(lambda x: x).map(lambda d: d[:10]).distinct().collect()
            unprocessed = [utc for utc in utc_dates if utc not in logged_utcs]
            print(f"FUNCTION get_unprocessed_utcs_for_table: [DEBUG] Unprocessed UTCs: {unprocessed}")
            return unprocessed

    except Exception as e:
        print(f"FUNCTION get_unprocessed_utcs_for_table: [ERROR] Failed in get_unprocessed_utcs_for_table for {table_name}")
        traceback.print_exc()
        return []

def load_table(table_name, key_column):
    from pyspark.sql.functions import col, lit
    from datetime import datetime
    import traceback

    start_time_total = datetime.now()
    print(f"\n====== FUNCTION load_table: [START] Loading table: {table_name} ======")

    try:
        copy_table_name = f"{table_name}_HIST"
        temp_table_name = f"{copy_table_name}_TMP"
        delete_table_name = f"{table_name}_HIST_DELETE"
        delete_temp_table_name = f"{delete_table_name}_TMP"

        # Read Delta-based staging tables
        insert_df = spark.read.table(f"{table_name}_INSERT")
        update_df = spark.read.table(f"{table_name}_UPDATE")
        delete_df = spark.read.table(f"{table_name}_DELETE")
        print("[INFO] Loaded INSERT, UPDATE, DELETE staging data")

        # Extract UTC timestamps
        utc_timestamps = sorted(set(
            insert_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect() +
            update_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect() +
            delete_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect()
        ))

        if not utc_timestamps:
            print(f" FUNCTION load_table: [SKIP] No UTC timestamps found for {table_name}")
            return

        # Filter out already processed UTCs
        try:
            main_log_df = spark.read.table("final_history_logs")
            processed_utcs = main_log_df.filter(
                (col("table_name") == table_name) & col("status").isin("D", "I", "U")
            ).select("UTCTimestamp").rdd.flatMap(lambda x: x).collect()
            utc_timestamps = [utc for utc in utc_timestamps if utc not in set(processed_utcs)]
        except:
            print("FUNCTION load_table:[INFO] Log table not found. Proceeding with all UTCs.")

        if not utc_timestamps:
            print(f"FUNCTION load_table: [INFO] No new UTCs to process for {table_name}")
            return

        # Load or initialize the main history table
        try:
            copy_table = spark.read.table(copy_table_name)
        except:
            copy_table = spark.createDataFrame([], insert_df.schema.add("flag", "string"))
            print(f"FUNCTION load_table: [INFO] Initialized empty history table: {copy_table_name}")

        for utc in utc_timestamps:
            print(f"\n-- FUNCTION load_table: [PROCESSING] {table_name} | UTC: {utc} --")
            try:
        
                insert_view = insert_df.filter(col("UTCTimestamp") == utc) \
                    .drop("insert_timestamp") \
                    .withColumn("flag", lit("A")) \
                    .withColumn("insert_timestamp", current_timestamp())

                update_view = update_df.filter(col("UTCTimestamp") == utc) \
                    .drop("insert_timestamp") \
                    .withColumn("flag", lit("A")) \
                    .withColumn("insert_timestamp", current_timestamp())

                delete_view = delete_df.filter(col("UTCTimestamp") == utc)

                # Handle deletes
                if not delete_view.rdd.isEmpty():
                    print(" FUNCTION load_table: [INFO] Processing deletes with key-based lookup...")
                    active_records = copy_table.filter(col("flag") == "A")
                    delete_keys = delete_view.select(col("row_number").alias(key_column)).distinct()

                    records_to_inactivate = active_records.join(delete_keys, key_column, "inner") \
                                                          .drop("flag").withColumn("flag", lit("I"))

                    # Append to history (with flag = 'I')
                    copy_table = copy_table.unionByName(records_to_inactivate)

                    # Ensure delete table exists
                    try:
                        spark.sql(f"""
                            CREATE TABLE IF NOT EXISTS {delete_table_name}
                            USING DELTA AS SELECT * FROM delete_view WHERE 1 = 0
                        """)
                    except:
                        pass

                    try:
                        # Read existing delete table (or empty)
                        try:
                            existing_delete_df = spark.read.table(delete_table_name)
                        except:
                            existing_delete_df = spark.createDataFrame([], delete_view.schema)

                        # Combine new delete_view and existing
                        combined_df = existing_delete_df.unionByName(delete_view, allowMissingColumns=True)

                        # Write combined to temp table
                        combined_df.write.mode("overwrite").saveAsTable(delete_temp_table_name)

                        # Atomic swap
                        spark.sql(f"DROP TABLE IF EXISTS {delete_table_name}")
                        spark.sql(f"ALTER TABLE {delete_temp_table_name} RENAME TO {delete_table_name}")

                        print(f"FUNCTION load_table: [INFO] Appended delete trigger rows to {delete_table_name} (via temp + rename)")

                    except Exception as delete_err:
                        print(f"FUNCTION load_table: [ERROR] Failed updating {delete_table_name}")
                        print(traceback.format_exc())

                # Handle inserts + updates
                merged_view = insert_view.unionByName(update_view, allowMissingColumns=True)
                if not merged_view.rdd.isEmpty():
                    print("FUNCTION load_table: [INFO] Appending inserts/updates...")
                    copy_table = copy_table.unionByName(merged_view, allowMissingColumns=True)
                    copy_table = add_hash_column(copy_table)

                # Write to temp history table and replace main
                copy_table.write.mode("overwrite").saveAsTable(temp_table_name)
                spark.sql(f"DROP TABLE IF EXISTS {copy_table_name}")
                spark.sql(f"ALTER TABLE {temp_table_name} RENAME TO {copy_table_name}")
                print(f"FUNCTION load_table: [INFO] Updated history table: {copy_table_name}")

                # Log all 3 operations
                end_time = datetime.now()
                update_log(table_name, "D", utc, start_time_total, end_time, delete_view.count())
                update_log(table_name, "I", utc, start_time_total, end_time, insert_view.count())
                update_log(table_name, "U", utc, start_time_total, end_time, update_view.count())

            except Exception as inner_e:
                print(f"FUNCTION load_table: [ERROR] UTC {utc} failed for table {table_name}")
                print(traceback.format_exc())
                update_log(table_name, "FAILURE", utc, start_time_total, datetime.now(), 0, traceback.format_exc())

        print(f"\n====== FUNCTION load_table: [COMPLETE] Table processed: {table_name} ======")

    except Exception as e:
        print(f"FUNCTION load_table: [FATAL] Table load failed: {table_name}")
        print(traceback.format_exc())
        update_log(table_name, "FAILURE", datetime.utcnow().isoformat(), start_time_total, datetime.now(), 0, traceback.format_exc())



# --- MAIN EXECUTION ---
print("[INFO] Starting execution")

try:
    table_list_df = spark.read.table("Tables_List")
    active_tables = [(row["TABLE_NAME"], row["KEY_COLUMN"]) for row in table_list_df.filter("STATUS = 'A'").collect()]
except:
    print("[ERROR] Failed to load Active_Tables_List")
    traceback.print_exc()
    raise RuntimeError("Failed to load active table list.")

try:
    main_log_df = spark.read.table("final_history_logs")
except:
    print("[INFO] Final log not found. Initializing empty DataFrame.")
    empty_schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("status", StringType(), True),
        StructField("load_date", DateType(), True),
        StructField("UTCTimestamp", StringType(), True),
        StructField("start_time", StringType(), True),
        StructField("end_time", StringType(), True),
        StructField("count", StringType(), True),
        StructField("error_message", StringType(), True),
    ])
    main_log_df = spark.createDataFrame([], empty_schema)

# --- TABLES TO RUN ---
tables_to_run = []
for table_name, key_column in active_tables:
    utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df, mode="FAILURE")
    if not utc_days:
        utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df, mode="ALL")
    if utc_days:
        tables_to_run.append((table_name, key_column))

print(f"[INFO] Active tables to run: {[x[0] for x in tables_to_run]}")

# --- EXECUTE IN PARALLEL ---
if tables_to_run:
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = {executor.submit(load_table, table, key): table for table, key in tables_to_run}
        for future in as_completed(futures):
            print(f"[THREAD COMPLETE] Table: {futures[future]}")
else:
    print("[INFO] No tables to run.")

# --- MERGE FINAL LOG ---
try:
    main_log_df = spark.read.table("final_history_logs")
    daily_log_df = spark.read.table("daily_history_logs")
except:
    print("[WARNING] Log table read failed. Initializing empty.")
    daily_log_df = spark.createDataFrame([], main_log_df.schema)

join_keys = ["table_name", "load_date", "UTCTimestamp", "status"]
cleaned_main_log_df = main_log_df.alias("main").join(
    daily_log_df.select(*join_keys).alias("daily"), on=join_keys, how="left_anti"
)
final_main_log_df = cleaned_main_log_df.unionByName(daily_log_df)

#display(final_main_log_df)

final_main_log_df.write.mode("overwrite").saveAsTable("final_history_logs")

spark.sql("DROP TABLE IF EXISTS daily_history_logs")
spark.sql("CREATE TABLE daily_history_logs AS SELECT * FROM final_history_logs WHERE 1=0")

print("[INFO] Execution finished.")
