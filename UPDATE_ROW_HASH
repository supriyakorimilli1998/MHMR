from pyspark.sql import SparkSession, functions as F
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, traceback

# --- INIT SPARK ---
spark = SparkSession.builder.appName("Update ROW_HASH for RAW tables").getOrCreate()
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

tz = pytz.timezone("America/Chicago")
def _ts(): return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

CATALOG = "MHMR_LAKEHOUSE"
RAW_SCHEMA = f"{CATALOG}.RAW"

# --- Exclude columns from hashing ---
EXCLUDE = {"ROW_HASH", "INSERT_TIMESTAMP", "UTCTimestamp", "STATUS"}

def _find_row_hash_name(cols):
    for c in cols:
        if c.upper() == "ROW_HASH":
            return c
    return None

def build_hash_df(df, key_col):
    """Return DataFrame with key and NEW_ROW_HASH column."""
    exclude_upper = {c.upper() for c in EXCLUDE}
    cols_keep = [f.name for f in df.schema.fields if f.name.upper() not in exclude_upper]

    if key_col not in cols_keep:
        cols_keep = [key_col] + [c for c in cols_keep if c != key_col]

    cleaned = []
    for c in cols_keep:
        cleaned.append(
            F.upper(
                F.regexp_replace(
                    F.when(F.col(c).isNull(), F.lit("")).otherwise(F.trim(F.col(c).cast("string"))),
                    r'[\u0000-\u001F\u007F-\u009F\u200B-\u200D\uFEFF]',
                    ""
                )
            ).alias(c)
        )

    tmp = df.select(*cleaned)
    ordered = sorted([c for c in tmp.columns if c != key_col])
    hash_input = F.concat_ws("||", F.col(key_col), *[F.col(c) for c in ordered]) if ordered else F.col(key_col)
    return tmp.select(F.col(key_col), F.md5(hash_input).alias("NEW_ROW_HASH"))

def update_row_hash(table_name, key_col):
    raw_tbl = f"{RAW_SCHEMA}.{table_name}_RAW"
    try:
        print(f"[{_ts()}] Reading RAW table: {raw_tbl}")
        df_raw = spark.read.table(raw_tbl)

        if key_col not in df_raw.columns:
            raise ValueError(f"Key column '{key_col}' not found in RAW table {raw_tbl}")

        hash_df = build_hash_df(df_raw, key_col)

        # Drop existing ROW_HASH, join new one
        row_hash_col = _find_row_hash_name(df_raw.columns)
        base_cols = [c for c in df_raw.columns if c != row_hash_col] if row_hash_col else df_raw.columns
        base_df = df_raw.select(*base_cols)

        joined = (
            base_df.alias("b")
            .join(hash_df.alias("h"), on=key_col, how="left")
            .withColumn("ROW_HASH", F.col("h.NEW_ROW_HASH"))
            .drop("NEW_ROW_HASH")
        )

        print(f"[{_ts()}] Writing updated RAW table: {raw_tbl}")
        (joined.write
               .format("delta")
               .mode("overwrite")
               .option("overwriteSchema", "true")
               .saveAsTable(raw_tbl))

        print(f"[{_ts()}] ✅ ROW_HASH updated for {raw_tbl}")
    except Exception as e:
        print(f"[{_ts()}] ❌ Failed {raw_tbl}: {e}")
        traceback.print_exc()

def run_update(max_workers=4):
    print(f"[{_ts()}] Reading TABLES_LIST …")
    # Expect TABLES_LIST to have TABLE_NAME and KEY_COLUMN
    tables_df = spark.read.table("TABLES_LIST")
    rows = [r.asDict() for r in tables_df.collect()]

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = [ex.submit(update_row_hash, r["TABLE_NAME"], r["KEY_COLUMN"]) for r in rows]
        for f in as_completed(futs):
            f.result()

if __name__ == "__main__":
    run_update(max_workers=4)
