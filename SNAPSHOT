# --- IMPORTS --- adding to get ins and updates counts
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws, when, lit, substring
from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import pytz
import os
import uuid
import traceback

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process Incremental Tables").getOrCreate()
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# --- CONFIG ---
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()
lock_file = "/tmp/update_log.lock"

# --- UTILITY FUNCTIONS ---
def release_lock():
    if os.path.exists(lock_file):
        os.remove(lock_file)

def add_hash_column(df):
    exclude_cols = {"ROW_HASH", "insert_timestamp", "UTCTimestamp", "STATUS"}
    cleaned_cols = [
        when(col(f.name).isNull(), lit(" ")).otherwise(col(f.name).cast("string")).alias(f.name)
        for f in df.schema.fields if f.name not in exclude_cols
    ]
    temp_df = df.select(*cleaned_cols)
    row_hash_col = sha2(concat_ws("||", *temp_df.columns), 256).alias("ROW_HASH")
    return df.withColumn("ROW_HASH", row_hash_col)

def safe_read_table(table_name):
    try:
        return spark.read.table(table_name)
    except Exception as e:
        print(f"[WARNING] Failed to read table: {table_name}")
        traceback.print_exc()
        return None

def update_log(table_name, status, utc_timestamp_str, start_time, end_time, count="0", error_message=""):
    print(f"[LOG] Updating log for table: {table_name}, status: {status}, UTC: {utc_timestamp_str}")

    run_date = datetime.now(pytz.timezone("America/Chicago")).date()

    new_row = [(str(table_name), 
                str(status), 
                run_date,
                str(utc_timestamp_str),
                str(start_time.isoformat()), 
                str(end_time.isoformat()), 
                str(count), 
                str(error_message))
                ]

    print(f"LOG_NEW_ROW: {new_row}")

    df_logs = spark.read.table("snapshot_daily_incremental_logs")
   # df_logs.printSchema()

    schema = spark.read.table("snapshot_daily_incremental_logs").schema
    new_df = spark.createDataFrame(new_row, schema)

    def acquire_lock(timeout=30):
        import time
        start = time.time()
        while os.path.exists(lock_file):
            if time.time() - start > timeout:
                return False
            time.sleep(0.5)
        with open(lock_file, 'w') as f:
            f.write("locked")
        return True

    if not acquire_lock():
        print("[LOCK] Could not acquire lock. Skipping log update.")
        return

    try:
        try:
            existing_df = spark.read.table("snapshot_daily_incremental_logs")
            filtered_df = existing_df.filter(~(
                (col("table_name") == table_name) &
                (col("load_date") == run_date) &
                (col("UTCTimestamp") == utc_timestamp_str) &
                (col("status") == status)
            ))
            final_df = filtered_df.unionByName(new_df)
        except:
            final_df = new_df

        final_df.write.mode("overwrite").saveAsTable("snapshot_daily_incremental_logs")
        print("[LOG] Log updated successfully.")
        
    except Exception:
        print("[ERROR] Exception in update_log()")
        traceback.print_exc()
    finally:
        release_lock()

def get_unprocessed_utcs_for_table(table_name, log_df, mode="ALL"):

    print(f"[DEBUG] Getting unprocessed UTCs for: {table_name} | Mode: {mode}")

    try:
        insert_df = safe_read_table(f"{table_name}_INSERT")
        if insert_df is None:
            print(f"[INFO] INSERT table not found for {table_name}")
            return []

        utc_dates = insert_df.select(substring(col("UTCTimestamp"), 1, 10)) \
                             .distinct().rdd.flatMap(lambda x: x).collect()

        if mode == "FAILURE":
            failed_utcs = log_df.filter((col("table_name") == table_name) & (col("status") == "FAILURE")) \
                                .select("UTCTimestamp").rdd.flatMap(lambda x: x).map(lambda d: d[:10]).distinct().collect()
            print(f"[DEBUG] Failure UTCs: {failed_utcs}")
            return [utc for utc in utc_dates if utc in failed_utcs]
        else:
            logged_utcs = log_df.filter(col("table_name") == table_name) \
                                .select("UTCTimestamp").rdd.flatMap(lambda x: x).map(lambda d: d[:10]).distinct().collect()
            unprocessed = [utc for utc in utc_dates if utc not in logged_utcs]
            print(f"[DEBUG] Unprocessed UTCs: {unprocessed}")
            return unprocessed

    except Exception as e:
        print(f"[ERROR] Failed in get_unprocessed_utcs_for_table for {table_name}")
        traceback.print_exc()
        return []


def load_table(table_name, key_column):
    start_time_total = datetime.now()
    print(f"[START] Processing table: {table_name}")

    try:
        copy_table_name = f"{table_name}_INC"
        temp_table_name = f"{copy_table_name}_TMP"

        insert_df = spark.read.table(f"{table_name}_INSERT")
        update_df = spark.read.table(f"{table_name}_UPDATE")
        delete_df = spark.read.table(f"{table_name}_DELETE")
        print("[INFO] Loaded INSERT, UPDATE, DELETE staging data")

        utc_timestamps = sorted(set(
            insert_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect() +
            update_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect() +
            delete_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect()
        ))
        print(f"[INFO] UTCs to process for {table_name}: {utc_timestamps}")

        try:
            main_log_df = spark.read.table("snapshot_final_incremental_logs")
            processed_utcs = main_log_df.filter(
                (col("table_name") == table_name) & (col("status").isin("D", "I", "U"))
            ).select("UTCTimestamp").rdd.flatMap(lambda x: x).collect()
            utc_timestamps = [utc for utc in utc_timestamps if utc not in processed_utcs]
        except:
            print("[INFO] Final log unavailable or no previously processed UTCs")

        if not utc_timestamps:
            print(f"[INFO] No new UTCs to process for table: {table_name}")
            return

        try:
            copy_table = spark.read.table(copy_table_name)
        except:
            copy_table = spark.createDataFrame([], insert_df.schema.add("flag", "string"))
            print("[INFO] Initialized new copy table for", copy_table_name)

        for utc in utc_timestamps:
            print(f"[UTC START] {utc} | Table: {table_name}")
            try:
                insert_view = insert_df.filter(col("UTCTimestamp") == utc)
                update_view = update_df.filter(col("UTCTimestamp") == utc)
                delete_view = delete_df.filter(col("UTCTimestamp") == utc)

                merged_view = insert_view.unionByName(update_view, allowMissingColumns=True) \
                    if not update_view.rdd.isEmpty() else insert_view

                if delete_view and not delete_view.rdd.isEmpty():
                    delete_keys = delete_view.select(col("row_number").alias(key_column)).distinct()
                    copy_table = copy_table.join(delete_keys, key_column, "left_anti")

                if not merged_view.rdd.isEmpty():
                    copy_table = copy_table.unionByName(merged_view, allowMissingColumns=True)

                # Logging (still inside loop)
                end_time = datetime.now()
                update_log(table_name, "I", utc, start_time_total, end_time, insert_view.count(), "")
                update_log(table_name, "U", utc, start_time_total, end_time, update_view.count(), "")

            except Exception as e:
                print(f"[ERROR] Failed to process UTC: {utc} for table: {table_name}")
                print(f"Exception {e}")
                traceback.print_exc()

        # After processing all UTCs, hash and write final table
        if not copy_table.rdd.isEmpty():
            copy_table = add_hash_column(copy_table)
            copy_table.write.mode("overwrite").saveAsTable(temp_table_name)
            spark.sql(f"DROP TABLE IF EXISTS {copy_table_name}")
            spark.sql(f"ALTER TABLE {temp_table_name} RENAME TO {copy_table_name}")
            spark.sql(f"DROP TABLE IF EXISTS {temp_table_name}")
            print(f"[INFO] Final write done for table: {copy_table_name}")

            # Print distinct UTCs present in the final table
        try:
            print(f"[VERIFY] Distinct UTCs in {copy_table_name} after processing:")
            spark.read.table(copy_table_name).select("UTCTimestamp").distinct().show(truncate=False)
        except Exception as e:
            print(f"[ERROR] Failed to read {copy_table_name} for UTC verification")
            traceback.print_exc()

    except Exception as e:
        print(f"[ERROR] Outer failure in load_table for {table_name}")
        print(f"Exception {e}")
        traceback.print_exc()



# --- MAIN EXECUTION ---
print("[INFO] Starting execution")

try:
    table_list_df = spark.read.table("Tables_List")
    active_tables = [(row["TABLE_NAME"], row["KEY_COLUMN"]) for row in table_list_df.filter("STATUS = 'A'").collect()]
except:
    print("[ERROR] Failed to load Active_Tables_List")
    traceback.print_exc()
    raise RuntimeError("Failed to load active table list.")

try:
    main_log_df = spark.read.table("snapshot_final_incremental_logs")
except:
    print("[INFO] Final log not found. Initializing empty DataFrame.")
    empty_schema = StructType([
        StructField("table_name", StringType(), True),
        StructField("status", StringType(), True),
        StructField("load_date", DateType(), True),
        StructField("UTCTimestamp", StringType(), True),
        StructField("start_time", StringType(), True),
        StructField("end_time", StringType(), True),
        StructField("count", StringType(), True),
        StructField("error_message", StringType(), True),
    ])
    main_log_df = spark.createDataFrame([], empty_schema)

# --- TABLES TO RUN ---
tables_to_run = []
for table_name, key_column in active_tables:
    utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df, mode="FAILURE")
    if not utc_days:
        utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df, mode="ALL")
    if utc_days:
        tables_to_run.append((table_name, key_column))

print(f"[INFO] Active tables to run: {[x[0] for x in tables_to_run]}")

# --- EXECUTE IN PARALLEL ---
if tables_to_run:
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = {executor.submit(load_table, table, key): table for table, key in tables_to_run}
        for future in as_completed(futures):
            print(f"[THREAD COMPLETE] Table: {futures[future]}")
else:
    print("[INFO] No tables to run.")

# --- MERGE FINAL LOG ---
try:
    main_log_df = spark.read.table("snapshot_final_incremental_logs")
    daily_log_df = spark.read.table("snapshot_daily_incremental_logs")
except:
    print("[WARNING] Log table read failed. Initializing empty.")
    daily_log_df = spark.createDataFrame([], main_log_df.schema)

join_keys = ["table_name", "load_date", "UTCTimestamp", "status"]
cleaned_main_log_df = main_log_df.alias("main").join(
    daily_log_df.select(*join_keys).alias("daily"), on=join_keys, how="left_anti"
)

final_main_log_df = cleaned_main_log_df.unionByName(daily_log_df)

display(final_main_log_df)

final_main_log_df.write.mode("overwrite").saveAsTable("snapshot_final_incremental_logs")

spark.sql("DROP TABLE IF EXISTS snapshot_daily_incremental_logs")
spark.sql("CREATE TABLE snapshot_daily_incremental_logs AS SELECT * FROM snapshot_final_incremental_logs WHERE 1=0")

print("[INFO] Execution finished.")
